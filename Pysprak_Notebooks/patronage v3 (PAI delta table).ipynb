{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4350febb-d9a5-4e2e-9f9f-05d90bcf2ca6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Libraries"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime, timedelta, date\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "28555b05-06d5-4138-9688-f676f4816a85",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define Schemas"
    }
   },
   "outputs": [],
   "source": [
    "# old_cg_schema = StructType(\n",
    "#     [\n",
    "#         StructField(\"Discharge_Revocation_Date__c\", StringType(), True),\n",
    "#         StructField(\"Caregiver_Status__c\", StringType(), True),\n",
    "#         StructField(\"CreatedById\", StringType(), True),\n",
    "#         StructField(\"Dispositioned_Date__c\", StringType(), True),\n",
    "#         StructField(\"Applicant_Type__c\", StringType(), True),\n",
    "#         StructField(\"CreatedDate\", StringType(), True),\n",
    "#         StructField(\"Veteran_ICN__c\", StringType(), True),\n",
    "#         StructField(\"Benefits_End_Date__c\", StringType(), True),\n",
    "#         StructField(\"Caregiver_Id__c\", StringType(), True),\n",
    "#         StructField(\"Caregiver_ICN__c\", StringType(), True),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "new_cg_schema = StructType(\n",
    "    [\n",
    "        StructField(\"Discharge_Revocation_Date__c\", StringType(), True),\n",
    "        StructField(\"Caregiver_Status__c\", StringType(), True),\n",
    "        StructField(\"CreatedById\", StringType(), True),\n",
    "        StructField(\"Dispositioned_Date__c\", StringType(), True),\n",
    "        StructField(\"CARMA_Case_ID__c\", StringType(), True),\n",
    "        StructField(\"Applicant_Type__c\", StringType(), True),\n",
    "        StructField(\"CreatedDate\", StringType(), True),\n",
    "        StructField(\"Veteran_ICN__c\", StringType(), True),\n",
    "        StructField(\"Benefits_End_Date__c\", StringType(), True),\n",
    "        StructField(\"Caregiver_Id__c\", StringType(), True),\n",
    "        StructField(\"CARMA_Case_Number__c\", StringType(), True),\n",
    "        StructField(\"Caregiver_ICN__c\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "scd_schema = StructType(\n",
    "    [\n",
    "        StructField(\"PTCPNT_ID\", StringType()),\n",
    "        StructField(\"FILE_NBR\", StringType()),\n",
    "        StructField(\"LAST_NM\", StringType()),\n",
    "        StructField(\"FIRST_NM\", StringType()),\n",
    "        StructField(\"MIDDLE_NM\", StringType()),\n",
    "        StructField(\"SUFFIX_NM\", StringType()),\n",
    "        StructField(\"STA_NBR\", StringType()),\n",
    "        StructField(\"BRANCH_OF_SERV\", StringType()),\n",
    "        StructField(\"DATE_OF_BIRTH\", StringType()),\n",
    "        StructField(\"DATE_OF_DEATH\", StringType()),\n",
    "        StructField(\"VET_SSN_NBR\", StringType()),\n",
    "        StructField(\"SVC_NBR\", StringType()),\n",
    "        StructField(\"AMT_GROSS_OR_NET_AWARD\", IntegerType()),\n",
    "        StructField(\"AMT_NET_AWARD\", IntegerType()),\n",
    "        StructField(\"NET_AWARD_DATE\", StringType()),\n",
    "        StructField(\"SPECL_LAW_IND\", IntegerType()),\n",
    "        StructField(\"VET_SSN_VRFCTN_IND\", IntegerType()),\n",
    "        StructField(\"WIDOW_SSN_VRFCTN_IND\", IntegerType()),\n",
    "        StructField(\"PAYEE_SSN\", StringType()),\n",
    "        StructField(\"ADDRS_ONE_TEXT\", StringType()),\n",
    "        StructField(\"ADDRS_TWO_TEXT\", StringType()),\n",
    "        StructField(\"ADDRS_THREE_TEXT\", StringType()),\n",
    "        StructField(\"ADDRS_CITY_NM\", StringType()),\n",
    "        StructField(\"ADDRS_ST_CD\", StringType()),\n",
    "        StructField(\"ADDRS_ZIP_PREFIX_NBR\", IntegerType()),\n",
    "        StructField(\"MIL_POST_OFFICE_TYP_CD\", StringType()),\n",
    "        StructField(\"MIL_POSTAL_TYPE_CD\", StringType()),\n",
    "        StructField(\"COUNTRY_TYPE_CODE\", IntegerType()),\n",
    "        StructField(\"SUSPENSE_IND\", IntegerType()),\n",
    "        StructField(\"PAYEE_NBR\", IntegerType()),\n",
    "        StructField(\"EOD_DT\", StringType()),\n",
    "        StructField(\"RAD_DT\", StringType()),\n",
    "        StructField(\"ADDTNL_SVC_IND\", StringType()),\n",
    "        StructField(\"ENTLMT_CD\", StringType()),\n",
    "        StructField(\"DSCHRG_PAY_GRADE_NM\", StringType()),\n",
    "        StructField(\"AMT_OF_OTHER_RETIREMENT\", IntegerType()),\n",
    "        StructField(\"RSRVST_IND\", StringType()),\n",
    "        StructField(\"NBR_DAYS_ACTIVE_RESRV\", IntegerType()),\n",
    "        StructField(\"CMBNED_DEGREE_DSBLTY\", StringType()),\n",
    "        StructField(\"DSBL_DTR_DT\", StringType()),\n",
    "        StructField(\"DSBL_TYP_CD\", StringType()),\n",
    "        StructField(\"VA_SPCL_PROV_CD\", IntegerType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "scd_schema1 = StructType(\n",
    "    [\n",
    "        StructField(\"PTCPNT_ID\", StringType()),\n",
    "        StructField(\"CMBNED_DEGREE_DSBLTY\", StringType()),\n",
    "        StructField(\"DSBL_DTR_DT\", StringType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pai_schema = StructType(\n",
    "    [\n",
    "        StructField(\"EDI_PI\", StringType()),\n",
    "        StructField(\"SSN_NBR\", StringType()),\n",
    "        StructField(\"FILE_NBR\", StringType()),\n",
    "        StructField(\"PTCPNT_VET_ID\", StringType()),\n",
    "        StructField(\"LAST_NM\", StringType()),\n",
    "        StructField(\"FIRST_NM\", StringType()),\n",
    "        StructField(\"MIDDLE_NM\", StringType()),\n",
    "        StructField(\"PT35_RATING_DT\", TimestampType()),\n",
    "        StructField(\"PT35_PRMLGN_DT\", TimestampType()),\n",
    "        StructField(\"PT35_EFFECTIVE_DATE\", TimestampType()),\n",
    "        StructField(\"PT35_END_DATE\", TimestampType()),\n",
    "        StructField(\"PT_35_FLAG\", StringType()),\n",
    "        StructField(\"COMBND_DEGREE_PCT\", StringType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "file_list_schema = StructType(\n",
    "    [\n",
    "        StructField(\"path\", StringType()),\n",
    "        StructField(\"name\", StringType()),\n",
    "        StructField(\"size\", StringType()),\n",
    "        StructField(\"modificationTime\", StringType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Loookup table for ICN, EDIPI\n",
    "# icn_relationship = (spark.read.format(\"delta\")\n",
    "#                     .load(\"dbfs:/FileStore/test_correlations\")\n",
    "icn_relationship = (spark.read.format(\"delta\")\n",
    "                    .load(\"/mnt/Patronage/identity_correlations\")\n",
    "                    .withColumnRenamed('MVIPersonICN', 'ICN')).persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d033a57e-2a9a-4d3c-99be-41aa70896f46",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Various conditions to use in upsert."
    }
   },
   "outputs": [],
   "source": [
    "# Define join conditions based on file type\n",
    "join_conditions = {\n",
    "    \"CG\": (\n",
    "        (col(\"ICN\") == col(\"target_ICN\"))\n",
    "        & (col(\"Veteran_ICN\") == col(\"target_Veteran_ICN\"))\n",
    "        & (col(\"Batch_CD\") == col(\"target_Batch_CD\"))\n",
    "        & (col(\"Applicant_Type\") == col(\"target_Applicant_Type\"))\n",
    "        & (col(\"target_RecordStatus\") == True)\n",
    "    ),\n",
    "    \"SCD\": (\n",
    "        (col(\"ICN\") == col(\"target_ICN\"))\n",
    "        & (col(\"target_RecordStatus\") == True)\n",
    "        & (col(\"Batch_CD\") == col(\"target_Batch_CD\"))\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Define delta condition based on file type\n",
    "delta_conditions = {\n",
    "    \"CG\": xxhash64(\n",
    "        col(\"Status_Begin_Date\"),\n",
    "        col(\"Status_Termination_Date\"),\n",
    "        col(\"Applicant_Type\"),\n",
    "        col(\"Caregiver_Status\")\n",
    "    ) != xxhash64(\n",
    "        col(\"target_Status_Begin_Date\"),\n",
    "        col(\"target_Status_Termination_Date\"),\n",
    "        col(\"target_Applicant_Type\"),\n",
    "        col(\"target_Caregiver_Status\")\n",
    "    ),\n",
    "    \"SCD\": xxhash64(\n",
    "        col(\"SC_Combined_Disability_Percentage\")\n",
    "    ) != xxhash64(col(\"target_SC_Combined_Disability_Percentage\")),\n",
    "}\n",
    "\n",
    "# Track changes in specified columns and create a change log\n",
    "columns_to_track = {\n",
    "    \"CG\": [\n",
    "        (\"Status_Begin_Date\", \"target_Status_Begin_Date\"),\n",
    "        (\"Status_Termination_Date\", \"target_Status_Termination_Date\"),\n",
    "        (\"Applicant_Type\", \"target_Applicant_Type\"),\n",
    "        (\"Caregiver_Status\", \"target_Caregiver_Status\")\n",
    "    ],\n",
    "    \"SCD\": [(\n",
    "        \"SC_Combined_Disability_Percentage\", \"target_SC_Combined_Disability_Percentage\"),\n",
    "    ],\n",
    "    \"PAI\": [\n",
    "        (\"PT_Indicator\", \"target_PT_Indicator\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Define merge conditions and batch_cd\n",
    "merge_conditions = {\n",
    "    \"CG\": \"concat(target.ICN, target.Veteran_ICN, target.Applicant_Type) = source.MERGEKEY and target.RecordStatus = True\",\n",
    "    \"SCD\": \"((target.ICN = source.MERGEKEY) and (target.Batch_CD = source.Batch_CD) and (target.RecordStatus = True))\"\n",
    "}\n",
    "\n",
    "# Define concat_column based on file type\n",
    "concat_column = {\n",
    "    \"CG\": concat(col(\"ICN\"), col(\"Veteran_ICN\"), col(\"Applicant_Type\")),\n",
    "    \"SCD\": col(\"ICN\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2d75a66-ed5c-4bff-9a14-ef264182e711",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Helper function to list required source files (get_all_files(path, start_time))"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_files(folder, cg_unix_start_time):\n",
    "    \"\"\"\n",
    "    Recursively lists files at any depth inside a directory based on filtering criteria.\n",
    "    Parameters:\n",
    "        path (str): Path of a directory.\n",
    "        cg_unix_start_time (int): Start time in Unix format for filtering files.\n",
    "    Returns:\n",
    "        List[Tuple]: List of files with metadata.\n",
    "    \"\"\"\n",
    "\n",
    "    for dir_path in dbutils.fs.ls(folder):\n",
    "        (dir_path.path, dir_path.name, dir_path.size, dir_path.modificationTime)\n",
    "        if dir_path.isFile():\n",
    "            if (\n",
    "                (\n",
    "                    dir_path.name.startswith(\"caregiverevent\")\n",
    "                    and dir_path.name.endswith(\".csv\")\n",
    "                    and dir_path.modificationTime > cg_unix_start_time\n",
    "                )\n",
    "                or (\n",
    "                    dir_path.name.startswith(\"CPIDODIEX_\")\n",
    "                    and dir_path.name.endswith(\".csv\")\n",
    "                    and \"NEW\" not in dir_path.name\n",
    "                )\n",
    "                or (dir_path.name.startswith(\"WRTS\") and dir_path.name.endswith(\".txt\"))\n",
    "                or (dir_path.path.startswith(\"dbfs:/mnt/vac20sdpasa201vba/ci-vba-edw-2/\") and dir_path.name.endswith(\"parquet\"))\n",
    "            ):\n",
    "                yield dir_path\n",
    "        elif dir_path.isDir() and folder != dir_path.path:\n",
    "            yield from get_all_files(dir_path.path, cg_unix_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4e01fcf-24b2-48e2-b1e0-e78bebd6e69a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Helper function converts datetime to  Unix time"
    }
   },
   "outputs": [],
   "source": [
    "def get_unix_time(year, month, day, hour=0, minute=0, second=0):\n",
    "    \"\"\"\n",
    "    Converts a datetime to Unix time in milliseconds.\n",
    "    \"\"\"\n",
    "    dt = datetime(year, month, day, hour, minute, second)\n",
    "    return int(time.mktime(dt.timetuple())) * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c39e8d1-8a70-4bce-8599-8889da472ad0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "New Logic Source Data Collection (collect_data_source())"
    }
   },
   "outputs": [],
   "source": [
    "def collect_data_source():\n",
    "    \"\"\"\n",
    "    Collects files from directories, filters them based on modification time, and returns a DataFrame.\n",
    "    Returns:\n",
    "        DataFrame: PySpark DataFrame with files ready to process.\n",
    "    \"\"\"\n",
    "\n",
    "    raw_file_folders = [\n",
    "        \"/mnt/ci-carma/landing/\",\n",
    "        \"/mnt/ci-vadir-shared/\",\n",
    "        \"/mnt/ci-patronage/pai_landing/\",\n",
    "        \"/mnt/vac20sdpasa201vba/ci-vba-edw-2/\",\n",
    "    ]\n",
    "\n",
    "    cg_unix_start_time = get_unix_time(2024, 12, 18, 23, 59, 59)\n",
    "    others_unix_start_time = get_unix_time(2023, 11, 1)\n",
    "\n",
    "    no_of_files = spark.sql(\n",
    "            \"SELECT COALESCE(COUNT(DISTINCT filename), 0) AS count FROM mypatronage_test\"\n",
    "        ).collect()[0][0]\n",
    "\n",
    "    query = (\n",
    "        f\"\"\"\n",
    "        SELECT COALESCE(MAX(SDP_Event_Created_Timestamp), TIMESTAMP('{others_unix_start_time}')) AS max_date \n",
    "        from mypatronage_test\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    max_processed_date = (\n",
    "        datetime.fromtimestamp(others_unix_start_time/1000)\n",
    "        if no_of_files == 1\n",
    "        else spark.sql(query).collect()[0][0]\n",
    "    )\n",
    "\n",
    "    now = datetime.now()\n",
    "    yesterday_end_time = (datetime(now.year, now.month, now.day) - timedelta(hours=4)) #Need to adjust to blob storage time\n",
    "    yesterday_end_time_ts = int(yesterday_end_time.timestamp() * 1000)\n",
    "\n",
    "    master_file_list = [\n",
    "        file for folder in raw_file_folders \n",
    "        for file in get_all_files(folder, cg_unix_start_time)\n",
    "    ]\n",
    "\n",
    "    file_list_df = spark.createDataFrame(master_file_list, file_list_schema)\n",
    "\n",
    "    filtered_file_list_df = (file_list_df\n",
    "                    .withColumn(\"dateTime\", to_timestamp(col(\"modificationTime\") / 1000))\n",
    "                    .filter(col(\"modificationTime\") > others_unix_start_time)\n",
    "                    .filter( (col(\"dateTime\") > max_processed_date) & (col(\"modificationTime\") <= yesterday_end_time_ts))\n",
    "    )\n",
    "\n",
    "    parquet_PT_flag = filtered_file_list_df.filter(filtered_file_list_df['path'].contains('parquet')).orderBy(desc(col(\"modificationTime\"))).limit(1)\n",
    "    # parquet_PT_flag is a reference that the delta table has been updated for PT_Indicator. \n",
    "    all_other_files = filtered_file_list_df.filter(~filtered_file_list_df['path'].contains('parquet'))\n",
    "\n",
    "    files_to_process_now = all_other_files.unionAll(parquet_PT_flag)\n",
    "\n",
    "    print(\n",
    "        f\"Upserting all files landed between date: {max_processed_date} and date: {yesterday_end_time}\"\n",
    "    )\n",
    "\n",
    "    if files_to_process_now.count() > 0:\n",
    "        return files_to_process_now.orderBy(col(\"modificationTime\"))\n",
    "    else:\n",
    "        dbutils.notebook.exit(\"Notebook exited because there are no files to upsert.\")\n",
    "        pass\n",
    "        # return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa87ac09-42b8-4e35-91b9-01e8e669f3a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This is the latest All Caregivers initial seed data\n",
    "  \n",
    "dbfs:/FileStore/All_Caregivers_InitialSeed_12182024_csv.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "742e04fe-46ba-4c70-8b21-c92642cc5f65",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "New: Load Caregivers seed file to a dataframe  (initialize_caregivers())"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_caregivers():\n",
    "        new_cg_df = spark.read.csv(\"dbfs:/FileStore/All_Caregivers_InitialSeed_12182024_csv.csv\", header=True, inferSchema=True) #use this dbfs:/FileStore/All_Caregivers_InitialSeed_12182024_csv.csv\n",
    "        transformed_cg_df = (new_cg_df\n",
    "                             .select(substring(\"ICN\", 1, 10).alias(\"ICN\"),  \n",
    "                                     \"Applicant_Type\", \"Caregiver_Status\", \n",
    "                                     date_format(\"Status_Begin_Date\",'yyyyMMdd').alias(\"Status_Begin_Date\"), \n",
    "                                     date_format(\"Status_Termination_Date\", 'yyyyMMdd').alias(\"Status_Termination_Date\"), \n",
    "                                     substring(\"Veteran_ICN\", 1, 10).alias(\"Veteran_ICN\"))\n",
    "        )\n",
    "\n",
    "        edipi_df = (\n",
    "                broadcast(transformed_cg_df)\n",
    "                .join(icn_relationship, [\"ICN\"], \"left\")\n",
    "                .withColumn(\"filename\", lit(\"dbfs:/FileStore/All_Caregivers_InitialSeed_12182024_csv.csv\"))\n",
    "                .withColumn(\"SDP_Event_Created_Timestamp\", lit('2024-12-18T23:59:59.000+00:00').cast(TimestampType())) # lit('2024-12-18T23:59:59.000+00:00')\n",
    "                .withColumn(\"Individual_Unemployability\", lit(None).cast(StringType()))\n",
    "                .withColumn(\"PT_Indicator\", lit(None).cast(StringType()))\n",
    "                .withColumn(\"SC_Combined_Disability_Percentage\", lit(None).cast(StringType()))\n",
    "                .withColumn(\"RecordStatus\", lit(True).cast(BooleanType()))\n",
    "                .withColumn(\"RecordLastUpdated\", lit(None).cast(DateType()))\n",
    "                .withColumn(\"Status_Last_Update\", lit(None).cast(StringType()))\n",
    "                .withColumn(\"sentToDoD\", lit(False).cast(BooleanType()))\n",
    "                .withColumn(\"Batch_CD\", lit(\"CG\").cast(StringType()))\n",
    "        )\n",
    "        return edipi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d4ab6bb-d976-4671-a1ba-541e7edae421",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Upsert Caregivers, PAI and IF8 (process_updates())"
    }
   },
   "outputs": [],
   "source": [
    "def process_updates(edipi_df, file_type):\n",
    "    \n",
    "    \"\"\"\n",
    "    Upserts the input dataframe depending on input file type ('CG', 'PAI or 'SCD'). Uses Slowly Changing Dimensions type 2 logic that stores records that have been updated.\n",
    "    Parameters: Pyspark dataframe and file type\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    # Optimize the table based on the event timestamp\n",
    "    spark.sql(\"OPTIMIZE mypatronage_test ZORDER BY (SDP_Event_Created_Timestamp)\")\n",
    "\n",
    "    # Load the target Delta table\n",
    "    targetTable = DeltaTable.forPath(spark, \"dbfs:/user/hive/warehouse/mypatronage_test\")\n",
    "    targetDF = targetTable.toDF().filter(\n",
    "        (col(\"Batch_CD\") == file_type) & (col(\"RecordStatus\") == True)\n",
    "    )\n",
    "    # Rename columns in targetDF for clarity\n",
    "    targetDF = targetDF.select([col(c).alias(f\"target_{c}\") for c in targetDF.columns])\n",
    "\n",
    "    # Perform the join based on file type\n",
    "    if file_type in [\"CG\", \"SCD\"]:\n",
    "        joinDF = broadcast(edipi_df).join(targetDF, join_conditions[file_type], \"leftouter\")\n",
    "\n",
    "        # Handling logic for SCD file type\n",
    "        if file_type == \"SCD\":\n",
    "            joinDF = (\n",
    "                joinDF.withColumn(\n",
    "                    \"Status_Last_Update\",\n",
    "                    col(\"DSBL_DTR_DT\")\n",
    "                )\n",
    "                .withColumn(\n",
    "                    \"Status_Begin_Date\",\n",
    "                    coalesce(col(\"target_Status_Begin_Date\"), col(\"DSBL_DTR_DT\"))\n",
    "                )\n",
    "                .withColumn(\n",
    "                    \"PT_Indicator\", coalesce(joinDF[\"target_PT_Indicator\"], lit(\"N\"))\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Filter records that have changes based on delta condition\n",
    "        filterDF = joinDF.filter(delta_conditions[file_type])\n",
    "\n",
    "        # Handle dummy records with null MERGEKEY for unmatched records\n",
    "        mergeDF = filterDF.withColumn(\"MERGEKEY\", concat_column[file_type])\n",
    "        dummyDF = filterDF.filter(col(\"target_ICN\").isNotNull()).withColumn(\"MERGEKEY\", lit(None))\n",
    "\n",
    "        # Union the filtered and dummy DataFrames\n",
    "        upsert_df = mergeDF.union(dummyDF)\n",
    "\n",
    "    if file_type == \"PAI\":\n",
    "        upsert_df = edipi_df \n",
    "\n",
    "    change_conditions = []\n",
    "    for source_col, target_col in columns_to_track[file_type]:\n",
    "        change_condition = when(\n",
    "            xxhash64(coalesce(col(source_col), lit(\"Null\"))) != xxhash64(coalesce(col(target_col), lit(\"Null\"))),\n",
    "            concat_ws(\n",
    "                \" \",\n",
    "                lit(source_col),\n",
    "                lit(\"old value:\"),\n",
    "                coalesce(col(target_col), lit(\"Null\")),\n",
    "                lit(\"changed to new value:\"),\n",
    "                coalesce(col(source_col), lit(\"Null\"))\n",
    "            )\n",
    "        ).otherwise(lit(\"\"))\n",
    "        change_conditions.append(change_condition)\n",
    "\n",
    "    new_record_condition = when(\n",
    "        col(\"target_icn\").isNull(), \n",
    "        lit(\"New Record\")\n",
    "        ).otherwise(lit(\"Updated Record\")\n",
    "        )\n",
    "\n",
    "    upsert_df = upsert_df.withColumn(\"RecordChangeStatus\", new_record_condition)\n",
    "\n",
    "    if len(change_conditions) > 0:\n",
    "        change_log_col = concat_ws(\" \", *[coalesce(cond, lit(\"\")) for cond in change_conditions])\n",
    "    else:\n",
    "        change_log_col = lit(\"\")\n",
    "\n",
    "    upsert_df = upsert_df.withColumn(\"change_log\", change_log_col) \n",
    "\n",
    "    if file_type == \"PAI\":\n",
    "        file_type = \"SCD\"\n",
    "    # Perform the merge operation\n",
    "    targetTable.alias(\"target\").merge(\n",
    "        upsert_df.alias(\"source\"),\n",
    "        merge_conditions[file_type]\n",
    "    ).whenMatchedUpdate(\n",
    "        set={\n",
    "            \"RecordStatus\": \"False\",\n",
    "            \"RecordLastUpdated\": \"source.SDP_Event_Created_Timestamp\",\n",
    "            \"sentToDoD\": \"true\",\n",
    "            \"RecordChangeStatus\": lit(\"Expired Record\")\n",
    "        }\n",
    "    ).whenNotMatchedInsert(\n",
    "        values={\n",
    "            \"edipi\": \"source.edipi\",\n",
    "            \"ICN\": \"source.ICN\",\n",
    "            \"Veteran_ICN\": \"source.Veteran_ICN\",\n",
    "            \"Applicant_Type\": \"source.Applicant_Type\",\n",
    "            \"Caregiver_Status\": \"source.Caregiver_Status\",\n",
    "            \"participant_id\": \"source.participant_id\",\n",
    "            \"Batch_CD\": \"source.Batch_CD\",\n",
    "            \"SC_Combined_Disability_Percentage\": \"source.SC_Combined_Disability_Percentage\",\n",
    "            \"PT_Indicator\": \"source.PT_Indicator\",\n",
    "            \"Individual_Unemployability\": \"source.Individual_Unemployability\",\n",
    "            \"Status_Begin_Date\": \"source.Status_Begin_Date\",\n",
    "            \"Status_Last_Update\": \"source.Status_Last_Update\",\n",
    "            \"Status_Termination_Date\": \"source.Status_Termination_Date\",\n",
    "            \"SDP_Event_Created_Timestamp\": \"source.SDP_Event_Created_Timestamp\",\n",
    "            \"RecordStatus\": \"true\",\n",
    "            \"RecordLastUpdated\": \"source.RecordLastUpdated\",\n",
    "            \"filename\": \"source.filename\",\n",
    "            \"sentToDoD\": \"false\",\n",
    "            \"change_log\": \"source.change_log\",\n",
    "            \"RecordChangeStatus\": \"source.RecordChangeStatus\"\n",
    "        }\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce51e4d5-efd4-4293-bfa3-3c45784ca37b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def prepare_caregivers_data(cg_csv_files):\n",
    "    \"\"\"\n",
    "    Filters caregivers filenames from input dataframe, aggregates data and returns dataframe\n",
    "    Parameters: Pyspark dataframe with all filenames and metadata that are not processed (upsert)\n",
    "    Returns: Dataframe: Dataframe with required column names and edipi of a caregiver ready for upsert\n",
    "    \"\"\"\n",
    "    print(f\"Upserting records from {cg_csv_files.count()} caregivers aggregated files\")\n",
    "    Window_Spec = Window.partitionBy(\"ICN\", \"Veteran_ICN\", \"Applicant_Type\").orderBy(\n",
    "        desc(\"Event_Created_Date\")\n",
    "    )\n",
    "\n",
    "    cg_csv_files_to_process = cg_csv_files.select(collect_list('path')).first()[0]\n",
    "    cg_df = (spark.read.schema(new_cg_schema).csv(cg_csv_files_to_process, header=True, inferSchema=False)\n",
    "                        .withColumn(\"filename\", col(\"_metadata.file_path\"))\n",
    "                        .withColumn(\"SDP_Event_Created_Timestamp\", (col(\"_metadata.file_modification_time\")).cast(TimestampType()))\n",
    "    )\n",
    "\n",
    "    combined_cg_df = (cg_df\n",
    "                      .select(\n",
    "                        substring(\"Caregiver_ICN__c\", 1, 10).alias(\"ICN\"),\n",
    "                        substring(\"Veteran_ICN__c\", 1, 10).alias(\"Veteran_ICN\"),\n",
    "                        date_format(\"Benefits_End_Date__c\", \"yyyyMMdd\").alias(\"Status_Termination_Date\").cast(StringType()),\n",
    "                        col(\"Applicant_Type__c\").alias(\"Applicant_Type\"),\n",
    "                        col(\"Caregiver_Status__c\").alias(\"Caregiver_Status\"),\n",
    "                        date_format(\"Dispositioned_Date__c\", \"yyyyMMdd\").alias(\"Status_Begin_Date\").cast(StringType()),\n",
    "                        col(\"CreatedDate\").cast(\"timestamp\").alias(\"Event_Created_Date\"),\n",
    "                        \"filename\",\n",
    "                        \"SDP_Event_Created_Timestamp\",\n",
    "                    )\n",
    "                ).filter(col(\"Caregiver_ICN__c\").isNotNull())\n",
    "\n",
    "    edipi_df = (\n",
    "        broadcast(combined_cg_df)\n",
    "        .join(icn_relationship, [\"ICN\"], \"left\")\n",
    "        .withColumn(\"Individual_Unemployability\", lit(None).cast(StringType()))\n",
    "        .withColumn(\"PT_Indicator\", lit(None).cast(StringType()))\n",
    "        .withColumn(\"SC_Combined_Disability_Percentage\", lit(None).cast(StringType()))\n",
    "        .withColumn(\"RecordStatus\", lit(True).cast(BooleanType()))\n",
    "        .withColumn(\"RecordLastUpdated\", lit(None).cast(DateType()))\n",
    "        .withColumn(\"Status_Last_Update\", lit(None).cast(StringType()))\n",
    "        .withColumn(\"sentToDoD\", lit(False).cast(BooleanType()))\n",
    "        .withColumn(\"Batch_CD\", lit(\"CG\").cast(StringType()))\n",
    "        .withColumn(\"rank\", rank().over(Window_Spec))\n",
    "        .filter(col(\"rank\") == 1)\n",
    "        .dropDuplicates().drop(\"rank\", \"va_profile_id\", \"record_updated_date\")\n",
    "    ).orderBy(col(\"Event_Created_Date\"))\n",
    "\n",
    "    return edipi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "346ea846-8fb8-4d52-8656-b8d7b8bb6554",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Prepare SCD data (IF8) (prepare_scd_data())"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_scd_data(row):\n",
    "    \"\"\"\n",
    "    Prepares SCD data from the input row. This is the disability % data.\n",
    "    Parameters: Row of data from pyspark dataframe with filenames and metadata that are not processed (upsert)\n",
    "    Returns: Dataframe: Dataframe with required column names and edipi of a Veteran ready for upsert\n",
    "    \"\"\"\n",
    "\n",
    "    file_name = row.path\n",
    "\n",
    "    print(f\"Upserting records from {file_name}\")\n",
    "\n",
    "    if len(spark.read.csv(file_name).columns) != 3:\n",
    "        schema = scd_schema\n",
    "    else:\n",
    "        schema = scd_schema1  # the schema changed  to 3 columns after May\n",
    "\n",
    "    scd_updates_df = (\n",
    "        spark.read.csv(file_name, schema=schema, header=True, inferSchema=False)\n",
    "        .selectExpr(\n",
    "            \"PTCPNT_ID as participant_id\", \"CMBNED_DEGREE_DSBLTY\", \"DSBL_DTR_DT\"\n",
    "        )\n",
    "        .withColumn(\"filename\", col(\"_metadata.file_path\"))\n",
    "        .withColumn(\"sentToDoD\", lit(False).cast(BooleanType()))\n",
    "        .withColumn(\"SDP_Event_Created_Timestamp\", (col(\"_metadata.file_modification_time\")).cast(TimestampType()),)\n",
    "        .withColumn(\"SC_Combined_Disability_Percentage\", \n",
    "                    lpad(coalesce( when(col(\"CMBNED_DEGREE_DSBLTY\") == \"\", lit(\"000\")).otherwise(col(\"CMBNED_DEGREE_DSBLTY\"))), 3, \"0\", ), )\n",
    "        .withColumn(\"DSBL_DTR_DT\", when(col(\"DSBL_DTR_DT\") == \"\", None).otherwise( date_format(to_date(col(\"DSBL_DTR_DT\"), \"MMddyyyy\"), \"yyyyMMdd\")), )\n",
    "    ).filter(col(\"DSBL_DTR_DT\").isNotNull())\n",
    "\n",
    "    Window_Spec = Window.partitionBy(scd_updates_df[\"participant_id\"]).orderBy(\n",
    "        desc(\"DSBL_DTR_DT\"), desc(\"SC_Combined_Disability_Percentage\")\n",
    "    )\n",
    "\n",
    "    edipi_df = (\n",
    "        broadcast(scd_updates_df)\n",
    "        .join(icn_relationship, [\"participant_id\"], \"left\")\n",
    "        .withColumn(\"rank\", rank().over(Window_Spec))\n",
    "        .withColumn(\"Veteran_ICN\", lit(None).cast(StringType()))\n",
    "        .withColumn(\"Applicant_Type\", lit(None).cast(StringType()))\n",
    "        .withColumn(\"Caregiver_Status\", lit(None).cast(StringType()))\n",
    "        .withColumn(\"Individual_Unemployability\", lit(None).cast(StringType()))\n",
    "        .withColumn(\"Status_Termination_Date\", lit(None).cast(StringType()))\n",
    "        .withColumn(\"RecordLastUpdated\", lit(None).cast(DateType()))\n",
    "        .withColumn(\"Batch_CD\", lit(\"SCD\"))\n",
    "        .withColumn(\"RecordStatus\", lit(True).cast(BooleanType()))\n",
    "        .filter(col(\"rank\") == 1)\n",
    "        .filter(col(\"ICN\").isNotNull())\n",
    "        .dropDuplicates()\n",
    "        .drop(\"rank\", \"va_profile_id\", \"record_updated_date\")\n",
    "    )\n",
    "\n",
    "    return edipi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1feb3832-2b10-40e2-9e27-3d47991a9c53",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Prepare PAI data (update_pai_data())"
    }
   },
   "outputs": [],
   "source": [
    "def update_pai_data(row, source_type):\n",
    "    \"\"\"\n",
    "    Prepares PT Indicator from the input row, transforms and updates a Veteran's PT_Indicator column in delta table. \n",
    "    Parameters: Row of data from pyspark dataframe with metadata that are not processed (upsert), \n",
    "                source_type is 'text' means its a static file (old source)\n",
    "                source_type is 'table' means its a delta table (new source)\n",
    "    Returns: Dataframe: Dataframe with required column names ready for upsert\n",
    "    \"\"\"\n",
    "\n",
    "    if source_type == 'text':\n",
    "        file_name = row.path\n",
    "        file_creation_dateTime = row.dateTime \n",
    "        raw_pai_df = (\n",
    "        spark.read.csv(file_name, header=True, inferSchema=True)\n",
    "        .withColumn(\n",
    "            \"SDP_Event_Created_Timestamp\",\n",
    "            (col(\"_metadata.file_modification_time\")).cast(TimestampType()),\n",
    "        )\n",
    "        .withColumn(\"filename\", col(\"_metadata.file_path\")))\n",
    "    elif source_type == 'table':\n",
    "        file_creation_dateTime = row.dateTime\n",
    "        file_name = f\"Updated from PA&I delta table on {file_creation_dateTime}\"\n",
    "        raw_pai_df = (spark.sql(\"\"\" SELECT * FROM DELTA.`/mnt/vac20sdpasa201vba/ci-vba-edw-2/DeltaTables/DW_ADHOC_RECURR.DOD_PATRONAGE_SCD_PT/` \"\"\"))\n",
    "\n",
    "    print(f\"Updating PT Indicator\")\n",
    "\n",
    "    targetTable = DeltaTable.forPath(spark, \"dbfs:/user/hive/warehouse/mypatronage_test\")\n",
    "    targetDF = (\n",
    "        targetTable.toDF().filter(\"Batch_CD == 'SCD'\").filter(\"RecordStatus=='True'\")\n",
    "    )\n",
    "    targetDF = targetDF.select([col(c).alias(f\"target_{c}\") for c in targetDF.columns])\n",
    "\n",
    "    pai_df = raw_pai_df.selectExpr(\n",
    "        \"PTCPNT_VET_ID as participant_id\", \"PT_35_FLAG as source_PT_Indicator\"\n",
    "    )\n",
    "    existing_pai_data_df = spark.sql(\n",
    "        \"SELECT participant_id, PT_Indicator from mypatronage_test where RecordStatus is True and Batch_CD = 'SCD'\"\n",
    "    )\n",
    "\n",
    "    joinDF = (\n",
    "        pai_df.join(\n",
    "            broadcast(targetDF),\n",
    "            pai_df[\"participant_id\"] == targetDF[\"target_participant_id\"],\n",
    "            \"left\",\n",
    "        )\n",
    "        .filter(targetDF[\"target_PT_Indicator\"] == \"N\")\n",
    "        .withColumn(\"filename\", lit(file_name))\n",
    "        .withColumn(\"SDP_Event_Created_Timestamp\", lit(file_creation_dateTime))\n",
    "    )\n",
    "\n",
    "    filterDF = joinDF.filter(\n",
    "        xxhash64(joinDF.source_PT_Indicator) != xxhash64(joinDF.target_PT_Indicator)\n",
    "    )\n",
    "\n",
    "    mergeDF = filterDF.withColumn(\"MERGEKEY\", filterDF.target_ICN)\n",
    "\n",
    "    dummyDF = filterDF.filter(\"target_ICN is not null\").withColumn(\n",
    "        \"MERGEKEY\", lit(None)\n",
    "    )\n",
    "\n",
    "    paiDF = mergeDF.union(dummyDF)\n",
    "\n",
    "    edipi_df = (paiDF.selectExpr(\n",
    "        \"target_edipi as edipi\",\n",
    "        \"participant_id\",\n",
    "        \"target_ICN\",\n",
    "        \"MERGEKEY\",\n",
    "        \"target_SC_Combined_Disability_Percentage as SC_Combined_Disability_Percentage\",\n",
    "        \"target_Status_Begin_Date as Status_Begin_Date\",\n",
    "        \"target_Status_Last_Update as Status_Last_Update\",\n",
    "        \"SDP_Event_Created_Timestamp\",\n",
    "        \"filename\",\"source_PT_Indicator\", \"target_PT_Indicator\"\n",
    "    )\n",
    "    .withColumn(\"ICN\", lit(col(\"target_ICN\")))\n",
    "    .withColumn(\"Veteran_ICN\", lit(None))\n",
    "    .withColumn(\"Applicant_Type\", lit(None))\n",
    "    .withColumn(\"Caregiver_Status\", lit(None))\n",
    "    .withColumn(\"Batch_CD\", lit(\"SCD\"))\n",
    "    .withColumn(\"PT_Indicator\", coalesce(col(\"source_PT_Indicator\"), lit(\"N\")))\n",
    "    .withColumn(\"Individual_Unemployability\", lit(None))\n",
    "    .withColumn(\"Status_Termination_Date\", lit(None))\n",
    "    .withColumn(\"RecordLastUpdated\", lit(None))\n",
    "    )\n",
    "    return(edipi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3690868a-6110-44f6-9970-b22d80c0c8ad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Processing unprocessed files (process_files())"
    }
   },
   "outputs": [],
   "source": [
    "def process_files(files_to_process_now):\n",
    "    \"\"\"\n",
    "    Seggregates files based on filename and calls required function to process these files\n",
    "    Parameters: Pyspark dataframe with filenames and metadata that are not processed\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    cg_csv_files = files_to_process_now.filter(files_to_process_now['path'].contains('caregiverevent'))\n",
    "    edipi_df = prepare_caregivers_data(cg_csv_files)\n",
    "    process_updates(edipi_df, \"CG\")\n",
    "    # display(cg_csv_files)\n",
    "\n",
    "    other_files = files_to_process_now.filter(~files_to_process_now['path'].contains('caregiverevent'))\n",
    "    other_rows = other_files.collect()\n",
    "\n",
    "    for row in other_rows:\n",
    "        filename = row.path\n",
    "        if \"CPIDODIEX\" in filename:\n",
    "            # print(f\"Now processing: {row.path}\")\n",
    "            edipi_df = prepare_scd_data(row)\n",
    "            process_updates(edipi_df, \"SCD\")\n",
    "        elif \"WRTS\" in filename:\n",
    "            # print(f\"Now processing: {row.path}\")\n",
    "            edipi_df = update_pai_data(row, \"text\")\n",
    "            process_updates(edipi_df, \"PAI\")\n",
    "        elif 'parquet' in filename:\n",
    "            # print(f\"Now processing: {row.path}\")\n",
    "            edipi_df = update_pai_data(row, \"table\")\n",
    "            process_updates(edipi_df, \"PAI\")\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22acde2b-bfc4-486a-ba55-704493b5bc29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\" CREATE TABLE IF NOT EXISTS mypatronage_test (edipi string, ICN string, Veteran_ICN string, participant_id string, Batch_CD string,  Applicant_Type string, Caregiver_Status string, SC_Combined_Disability_Percentage string, PT_Indicator string, Individual_Unemployability string, Status_Begin_Date string, Status_Last_Update string, Status_Termination_Date string, SDP_Event_Created_Timestamp timestamp, filename string, RecordLastUpdated date, RecordStatus boolean, sentToDoD boolean, change_log string, RecordChangeStatus string) PARTITIONED BY(Batch_CD, RecordStatus ) LOCATION 'dbfs:/user/hive/warehouse/mypatronage_test' \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e53bf44f-6e67-4a08-8c2b-21e423ab221a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Bundling everything (patronage())"
    }
   },
   "outputs": [],
   "source": [
    "def patronage():\n",
    "\n",
    "\n",
    "    file_counts = spark.sql(\n",
    "        \"SELECT COALESCE(COUNT(DISTINCT filename), 0) AS count from mypatronage_test\"\n",
    "    ).collect()[0][0]\n",
    "    print(f\"Total files processed till last run: {file_counts}\")\n",
    "\n",
    "    if file_counts == 0:\n",
    "        print(\"Loading seed file into delta table\")\n",
    "        seed_df = initialize_caregivers()\n",
    "        process_updates(seed_df, \"CG\")\n",
    "        print(\"Loading required files for upsert\")\n",
    "        files_to_process_now = collect_data_source()\n",
    "        print(f\"Total files to process in this run: {files_to_process_now.count()}...\")\n",
    "        process_files(files_to_process_now)\n",
    "    else:\n",
    "        print(\"Loading required files for upsert\")\n",
    "        files_to_process_now = collect_data_source()\n",
    "        print(f\"Total files to process in this run: {files_to_process_now.count()}...\")\n",
    "        process_files(files_to_process_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a383d787-7da9-444d-9918-acb6762f60cb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "patronage() function"
    }
   },
   "outputs": [],
   "source": [
    "patronage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73789eb0-7838-48af-b4af-fab2743971bc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Displaying Files to be upserted since the last run (Testing Code)"
    }
   },
   "outputs": [],
   "source": [
    "files_to_process_now = collect_data_source()\n",
    "\n",
    "display(files_to_process_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6bfa8e1-069f-464d-8ad5-21e75fb57afe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "files_to_process_now = collect_data_source()\n",
    "\n",
    "display(files_to_process_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19e71e59-5970-42c4-a808-21e70dc82ed2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "remaining_files = files_to_process_now.filter(files_to_process_now['path'].contains('parquet'))\n",
    "file_creation_dateTime = remaining_files.select(max(\"dateTime\")).collect()[0][0]\n",
    "print((file_creation_dateTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10c0a52c-5524-40f5-b952-f47f8be11423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT count(*)\n",
    "FROM mypatronage_test\n",
    "where ICN NOT RLIKE '^[0-9]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a776c1c-79e6-466a-a217-4151bd68c9c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "SCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54824a43-f47e-4f35-bbef-f3cdbe1953b8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Max and Min file processed date"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  BATCH_CD,\n",
    "  min(SDP_Event_Created_Timestamp) First_Processed_date,\n",
    "  max(SDP_Event_Created_Timestamp) Last_Processed_date\n",
    "FROM\n",
    "  mypatronage_test\n",
    "GROUP BY 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1aa8608-44cc-45fb-a9ed-07f888dac696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.sql(\"drop table mypatronage_test\")\n",
    "# dbutils.fs.rm('dbfs:/user/hive/warehouse/mypatronage_test', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "560b414f-7e2b-403b-8cbb-aefa4a2abdf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql select date_format(current_date(), 'yyyyMMdd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "007844cc-a9ff-42af-a9ba-dc9e36bd6663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT Batch_CD, count(*), (DATE(SDP_Event_Created_Timestamp)) \n",
    "FROM \n",
    "mypatronage_test\n",
    "-- DELTA.`/mnt/Patronage/SCD_Staging`\n",
    "-- DELTA.`/mnt/Patronage/Caregivers_Staging_New`\n",
    "WHERE EDIPI IS NOT NULL\n",
    "GROUP BY ALL\n",
    "ORDER BY 3 DESC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b526cc44-765e-41ce-9975-aa54735bb58e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT count(*), (DATE(SDP_Event_Created_Timestamp)) \n",
    "FROM DELTA.`/mnt/Patronage/SCD_Staging` \n",
    "GROUP BY ALL \n",
    "ORDER BY 2 DESC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47fc6c24-5f83-4661-b8f3-6d9c1772e65b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT DISTINCT RecordChangeStatus, count(*), SDP_Event_Created_Timestamp\n",
    "FROM   mypatronage_test\n",
    "WHERE SDP_Event_Created_Timestamp =  (SELECT DISTINCT SDP_Event_Created_Timestamp FROM mypatronage_test where BATCH_CD = 'SCD' ORDER BY 1 desc LIMIT 1 OFFSET 0)\n",
    "GROUP BY ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8293939f-97d5-42a9-ac00-6fc7a7f5b785",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT Applicant_Type as distinct_cg, count(*) \n",
    "FROM mypatronage_test \n",
    "WHERE BATCH_CD = 'CG' \n",
    "AND recordstatus is TRUE \n",
    "GROUP BY ALL \n",
    "ORDER BY 2 desc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff11ba2d-c73e-4c3b-a851-bdb946c4f592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT count(*) FROM DELTA.`/mnt/Patronage/SCD_Staging`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "403f269b-4a5a-4161-8e2d-1611bb9c83e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT  DISTINCT SC_Combined_Disability_Percentage, PT_Indicator, count(*) as Total_Count FROM DELTA.`/mnt/Patronage/SCD_Staging` group BY ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79242e2c-6369-4873-8e24-7c4040f81e0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT  DISTINCT PT_Indicator, count(*) as Total_Count FROM DELTA.`/mnt/Patronage/SCD_Staging` group BY ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5054b07-96c7-4071-a67a-baca5a48664e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT DISTINCT caregiver_status FROM mypatronage_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c04d3522-9cce-4ecc-8e84-561836409d1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"dbfs:/mnt/ci-patronage/pai_landing/pt-indicator-6-6-24\", schema=pai_schema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ef10d4e-204e-46b2-abef-015d073db136",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "571b26a9-c4c3-439a-a31b-fed784e3d73f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SELECT * FROM DELTA.`/mnt/vac20sdpasa201vba/ci-vba-edw-2/DeltaTables/DW_ADHOC_RECURR.DOD_PATRONAGE_SCD_PT/` where PTCPNT_VET_ID = 14337381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d6c782a-529b-4e46-b9dd-c83ac358f805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM (DESCRIBE HISTORY mypatronage_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af6f0763-74a6-445b-8ed2-dbef26443511",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- RESTORE TABLE mypatronage_test VERSION AS OF 126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bedd7c1e-1cb7-4197-8cef-e44242d31b52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT count(*)\n",
    "FROM mypatronage_test\n",
    "WHERE Batch_CD = 'SCD'\n",
    "AND filename = \"Updated from PA&I delta table on 2025-01-10 18:40:52\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd8feedc-d4c2-46f4-b65c-75d1bacd4866",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"/mnt/Patronage/identity_correlations\", True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4275253824152717,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "patronage v3 (PAI delta table)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
