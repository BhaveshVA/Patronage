{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdbaa5ea-5d66-44e0-87c1-0a388e8cffc2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Importing Libraries"
    }
   },
   "outputs": [],
   "source": [
    "# /mnt/Patronage/identity_correlations\n",
    "from delta.tables import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime, timedelta, date\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fedf12bd-6dbf-44ca-bdd0-806a70e411fe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Widget to name Patronage delta table and the Path of the table"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"PatronageDeltaTableName\",\"mypatronage_test111\", \"Patronage Delta Table Name\")\n",
    "dbutils.widgets.text(\"PatronageDeltaTablePath\",\"dbfs:/user/hive/warehouse/\", \"Patronage Delta Table Path\")\n",
    "patronage_tablename = dbutils.widgets.get(\"PatronageDeltaTableName\")\n",
    "patronage_table_location = dbutils.widgets.get(\"PatronageDeltaTablePath\")\n",
    "fullname = patronage_table_location + patronage_tablename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "00183229-72b4-4446-b321-b4da15e923e9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Widget to Rebuild Patronage Delta Table"
    }
   },
   "outputs": [],
   "source": [
    "# Choose True to rebuild Patronage Delta Table\n",
    "dbutils.widgets.dropdown(\"rebuild\", \"False\", [\"True\", \"False\"], \"1 Rebuild Patronage\")\n",
    "boolean_value = dbutils.widgets.get(\"rebuild\")\n",
    "\n",
    "if boolean_value == 'True':\n",
    "    dropTableQuery = f\"\"\" DROP TABLE IF EXISTS {patronage_tablename} \"\"\"\n",
    "    spark.sql(dropTableQuery)\n",
    "    dbutils.fs.rm(fullname, True)\n",
    "else:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84f90464-c223-4d33-b519-f97256ae3459",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating Variables and Patronage delta table"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"CaregiverSource\", \"/mnt/ci-carma/landing/\", \"Caregiver Source\")\n",
    "dbutils.widgets.text(\"DisabilityPercentSource\", \"/mnt/ci-vadir-shared/\", \"Disability Percent Source\")\n",
    "dbutils.widgets.text(\"OldPTIndicatorSource\", \"/mnt/ci-patronage/pai_landing/\", \"Old PT Indicator Source\")\n",
    "dbutils.widgets.text(\"NewPTIndicatorSource\", \"/mnt/ci-vba-edw-2/DeltaTables/DW_ADHOC_RECURR.DOD_PATRONAGE_SCD_PT/\", \"New PT Indicator Source\")\n",
    "\n",
    "dbutils.widgets.text(\"StartDateToProcessCaregiversData\",\"2024-12-18 23:59:59\", \"Start Date to Process Caregivers Data\")\n",
    "dbutils.widgets.text(\"StartDateToProcessSCDData\",\"2025-04-01 00:00:00\", \"Start Date to Process SCD Data\")\n",
    "dbutils.widgets.text(\"PathToInitialSeedCaregiversCSVFile\", \"dbfs:/FileStore/All_Caregivers_InitialSeed_12182024_csv.csv\", \"Path to initial seed Caregivers csv file\")\n",
    "dbutils.widgets.text(\"PathToIdentityCorrelationsDeltaTable\", \"/mnt/Patronage/identity_correlations\", \"Path to Identity Correlations Delta Table\")\n",
    "initial_cg_file = dbutils.widgets.get(\"PathToInitialSeedCaregiversCSVFile\")\n",
    "cg_source = dbutils.widgets.get(\"CaregiverSource\")\n",
    "scd_source = dbutils.widgets.get(\"DisabilityPercentSource\")\n",
    "pt_old_source = dbutils.widgets.get(\"OldPTIndicatorSource\")\n",
    "pt_new_source = dbutils.widgets.get(\"NewPTIndicatorSource\")\n",
    "\n",
    "if not DeltaTable.isDeltaTable(spark, fullname):\n",
    "        print(f\"Creating {patronage_tablename}, on location {fullname}\")\n",
    "        create_table_query = f\"\"\"CREATE TABLE IF NOT EXISTS {patronage_tablename} (edipi string, ICN string, Veteran_ICN string, participant_id string, Batch_CD string,  Applicant_Type string, Caregiver_Status string, SC_Combined_Disability_Percentage string, PT_Indicator string, Individual_Unemployability string, Status_Begin_Date string, Status_Last_Update string, Status_Termination_Date string, SDP_Event_Created_Timestamp timestamp, filename string, RecordLastUpdated date, RecordStatus boolean, sentToDoD boolean, change_log string, RecordChangeStatus string) PARTITIONED BY(Batch_CD, RecordStatus ) LOCATION '{fullname}' \"\"\"\n",
    "        spark.sql(create_table_query)\n",
    "        file_count_query = f\"SELECT COALESCE(COUNT(DISTINCT filename), 0) AS count FROM DELTA.`{fullname}`\"\n",
    "        no_of_files = spark.sql(file_count_query).collect()[0][0]\n",
    "        print(f\"Total files processed till last run: {no_of_files}\")\n",
    "else:\n",
    "        file_count_query = f\"SELECT COALESCE(COUNT(DISTINCT filename), 0) AS count FROM DELTA.`{fullname}`\"\n",
    "        no_of_files = spark.sql(file_count_query).collect()[0][0]\n",
    "        print(f\"Total files processed till last run: {no_of_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b843903b-a19c-4984-abfd-0f59fabb2d69",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Defining necessary schemas"
    }
   },
   "outputs": [],
   "source": [
    "new_cg_schema = StructType(\n",
    "    [\n",
    "        StructField(\"Discharge_Revocation_Date__c\", StringType(), True),\n",
    "        StructField(\"Caregiver_Status__c\", StringType(), True),\n",
    "        StructField(\"CreatedById\", StringType(), True),\n",
    "        StructField(\"Dispositioned_Date__c\", StringType(), True),\n",
    "        StructField(\"CARMA_Case_ID__c\", StringType(), True),\n",
    "        StructField(\"Applicant_Type__c\", StringType(), True),\n",
    "        StructField(\"CreatedDate\", StringType(), True),\n",
    "        StructField(\"Veteran_ICN__c\", StringType(), True),\n",
    "        StructField(\"Benefits_End_Date__c\", StringType(), True),\n",
    "        StructField(\"Caregiver_Id__c\", StringType(), True),\n",
    "        StructField(\"CARMA_Case_Number__c\", StringType(), True),\n",
    "        StructField(\"Caregiver_ICN__c\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "scd_schema = StructType(\n",
    "    [\n",
    "        StructField(\"PTCPNT_ID\", StringType()),\n",
    "        StructField(\"FILE_NBR\", StringType()),\n",
    "        StructField(\"LAST_NM\", StringType()),\n",
    "        StructField(\"FIRST_NM\", StringType()),\n",
    "        StructField(\"MIDDLE_NM\", StringType()),\n",
    "        StructField(\"SUFFIX_NM\", StringType()),\n",
    "        StructField(\"STA_NBR\", StringType()),\n",
    "        StructField(\"BRANCH_OF_SERV\", StringType()),\n",
    "        StructField(\"DATE_OF_BIRTH\", StringType()),\n",
    "        StructField(\"DATE_OF_DEATH\", StringType()),\n",
    "        StructField(\"VET_SSN_NBR\", StringType()),\n",
    "        StructField(\"SVC_NBR\", StringType()),\n",
    "        StructField(\"AMT_GROSS_OR_NET_AWARD\", IntegerType()),\n",
    "        StructField(\"AMT_NET_AWARD\", IntegerType()),\n",
    "        StructField(\"NET_AWARD_DATE\", StringType()),\n",
    "        StructField(\"SPECL_LAW_IND\", IntegerType()),\n",
    "        StructField(\"VET_SSN_VRFCTN_IND\", IntegerType()),\n",
    "        StructField(\"WIDOW_SSN_VRFCTN_IND\", IntegerType()),\n",
    "        StructField(\"PAYEE_SSN\", StringType()),\n",
    "        StructField(\"ADDRS_ONE_TEXT\", StringType()),\n",
    "        StructField(\"ADDRS_TWO_TEXT\", StringType()),\n",
    "        StructField(\"ADDRS_THREE_TEXT\", StringType()),\n",
    "        StructField(\"ADDRS_CITY_NM\", StringType()),\n",
    "        StructField(\"ADDRS_ST_CD\", StringType()),\n",
    "        StructField(\"ADDRS_ZIP_PREFIX_NBR\", IntegerType()),\n",
    "        StructField(\"MIL_POST_OFFICE_TYP_CD\", StringType()),\n",
    "        StructField(\"MIL_POSTAL_TYPE_CD\", StringType()),\n",
    "        StructField(\"COUNTRY_TYPE_CODE\", IntegerType()),\n",
    "        StructField(\"SUSPENSE_IND\", IntegerType()),\n",
    "        StructField(\"PAYEE_NBR\", IntegerType()),\n",
    "        StructField(\"EOD_DT\", StringType()),\n",
    "        StructField(\"RAD_DT\", StringType()),\n",
    "        StructField(\"ADDTNL_SVC_IND\", StringType()),\n",
    "        StructField(\"ENTLMT_CD\", StringType()),\n",
    "        StructField(\"DSCHRG_PAY_GRADE_NM\", StringType()),\n",
    "        StructField(\"AMT_OF_OTHER_RETIREMENT\", IntegerType()),\n",
    "        StructField(\"RSRVST_IND\", StringType()),\n",
    "        StructField(\"NBR_DAYS_ACTIVE_RESRV\", IntegerType()),\n",
    "        StructField(\"CMBNED_DEGREE_DSBLTY\", StringType()),\n",
    "        StructField(\"DSBL_DTR_DT\", StringType()),\n",
    "        StructField(\"DSBL_TYP_CD\", StringType()),\n",
    "        StructField(\"VA_SPCL_PROV_CD\", IntegerType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "scd_schema1 = StructType(\n",
    "    [\n",
    "        StructField(\"PTCPNT_ID\", StringType()),\n",
    "        StructField(\"CMBNED_DEGREE_DSBLTY\", StringType()),\n",
    "        StructField(\"DSBL_DTR_DT\", StringType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pai_schema = StructType(\n",
    "    [\n",
    "        StructField(\"EDI_PI\", StringType()),\n",
    "        StructField(\"SSN_NBR\", StringType()),\n",
    "        StructField(\"FILE_NBR\", StringType()),\n",
    "        StructField(\"PTCPNT_VET_ID\", StringType()),\n",
    "        StructField(\"LAST_NM\", StringType()),\n",
    "        StructField(\"FIRST_NM\", StringType()),\n",
    "        StructField(\"MIDDLE_NM\", StringType()),\n",
    "        StructField(\"PT35_RATING_DT\", TimestampType()),\n",
    "        StructField(\"PT35_PRMLGN_DT\", TimestampType()),\n",
    "        StructField(\"PT35_EFFECTIVE_DATE\", TimestampType()),\n",
    "        StructField(\"PT35_END_DATE\", TimestampType()),\n",
    "        StructField(\"PT_35_FLAG\", StringType()),\n",
    "        StructField(\"COMBND_DEGREE_PCT\", StringType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "file_list_schema = StructType(\n",
    "    [\n",
    "        StructField(\"path\", StringType()),\n",
    "        StructField(\"name\", StringType()),\n",
    "        StructField(\"size\", StringType()),\n",
    "        StructField(\"modificationTime\", StringType()),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9133237d-d871-4495-a16f-def76ab0fc44",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Defining various conditions  for Patronage data"
    }
   },
   "outputs": [],
   "source": [
    "# Define join conditions based on file type\n",
    "join_conditions = {\n",
    "    \"CG\": (\n",
    "        (col(\"ICN\") == col(\"target_ICN\"))\n",
    "        & (col(\"Veteran_ICN\") == col(\"target_Veteran_ICN\"))\n",
    "        & (col(\"Batch_CD\") == col(\"target_Batch_CD\"))\n",
    "        & (col(\"Applicant_Type\") == col(\"target_Applicant_Type\"))\n",
    "        & (col(\"target_RecordStatus\") == True)\n",
    "    ),\n",
    "    \"SCD\": (\n",
    "        (col(\"ICN\") == col(\"target_ICN\"))\n",
    "        & (col(\"target_RecordStatus\") == True)\n",
    "        & (col(\"Batch_CD\") == col(\"target_Batch_CD\"))\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Define delta condition based on file type\n",
    "delta_conditions = {\n",
    "    \"CG\": xxhash64(\n",
    "        col(\"Status_Begin_Date\"),\n",
    "        col(\"Status_Termination_Date\"),\n",
    "        col(\"Applicant_Type\"),\n",
    "        col(\"Caregiver_Status\")\n",
    "    ) != xxhash64(\n",
    "        col(\"target_Status_Begin_Date\"),\n",
    "        col(\"target_Status_Termination_Date\"),\n",
    "        col(\"target_Applicant_Type\"),\n",
    "        col(\"target_Caregiver_Status\")\n",
    "    ),\n",
    "    \"SCD\": xxhash64(\n",
    "        col(\"SC_Combined_Disability_Percentage\")\n",
    "    ) != xxhash64(col(\"target_SC_Combined_Disability_Percentage\")),\n",
    "}\n",
    "\n",
    "# Track changes in specified columns and create a change log\n",
    "columns_to_track = {\n",
    "    \"CG\": [\n",
    "        (\"Status_Begin_Date\", \"target_Status_Begin_Date\"),\n",
    "        (\"Status_Termination_Date\", \"target_Status_Termination_Date\"),\n",
    "        (\"Applicant_Type\", \"target_Applicant_Type\"),\n",
    "        (\"Caregiver_Status\", \"target_Caregiver_Status\")\n",
    "    ],\n",
    "    \"SCD\": [(\n",
    "        \"SC_Combined_Disability_Percentage\", \"target_SC_Combined_Disability_Percentage\"),\n",
    "    ],\n",
    "    \"PAI\": [\n",
    "        (\"PT_Indicator\", \"target_PT_Indicator\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Define merge conditions and batch_cd\n",
    "merge_conditions = {\n",
    "    \"CG\": \"concat(target.ICN, target.Veteran_ICN, target.Applicant_Type) = source.MERGEKEY and target.RecordStatus = True\",\n",
    "    \"SCD\": \"((target.ICN = source.MERGEKEY) and (target.Batch_CD = source.Batch_CD) and (target.RecordStatus = True))\"\n",
    "}\n",
    "\n",
    "# Define concat_column based on file type\n",
    "concat_column = {\n",
    "    \"CG\": concat(col(\"ICN\"), col(\"Veteran_ICN\"), col(\"Applicant_Type\")),\n",
    "    \"SCD\": col(\"ICN\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5436e1d-b857-46e9-a07f-5a0811b1d291",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating FileProcessor Class and other methods for Caregivers and SCD"
    }
   },
   "outputs": [],
   "source": [
    "class FileProcessor:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.initial_cg_file = initial_cg_file\n",
    "        self.cg_source = cg_source\n",
    "        self.scd_source = scd_source\n",
    "        self.pt_old_source = pt_old_source\n",
    "        self.pt_new_source = pt_new_source\n",
    "        self.patronage_tablename = patronage_tablename\n",
    "        self.patronage_table_location = patronage_table_location\n",
    "        self.fullname = fullname\n",
    "        self.no_of_files = no_of_files\n",
    "        self.cg_start_datetime = dbutils.widgets.get(\"StartDateToProcessCaregiversData\")\n",
    "        self.others_start_datetime = dbutils.widgets.get(\"StartDateToProcessSCDData\")\n",
    "        self.identity_correlations_path = dbutils.widgets.get(\"PathToIdentityCorrelationsDeltaTable\")\n",
    "        self.icn_relationship = (spark.read.format(\"delta\")\n",
    "                    .load(self.identity_correlations_path)\n",
    "                    .withColumnRenamed('MVIPersonICN', 'ICN')).persist()\n",
    "\n",
    "    def source_directories(self):\n",
    "\n",
    "        raw_file_folders = list((self.cg_source, self.scd_source, self.pt_old_source, self.pt_new_source))\n",
    "\n",
    "        return raw_file_folders\n",
    "    \n",
    "    def get_all_files(self, raw_file_folders, cg_unix_start_time, others_unix_start_time):\n",
    "        \"\"\"\n",
    "        Recursively lists files at any depth inside a directory based on filtering criteria.\n",
    "        Parameters:\n",
    "        path (str): Path of a directory.\n",
    "        cg_unix_start_time (int): Start time in Unix format for filtering files.\n",
    "        Returns:\n",
    "        List[Tuple]: List of files with metadata.\n",
    "        \"\"\"\n",
    "        self.raw_file_folders = raw_file_folders\n",
    "        self.cg_unix_start_time = cg_unix_start_time\n",
    "        self.others_unix_start_time = others_unix_start_time\n",
    "        for dir_path in dbutils.fs.ls(self.raw_file_folders):\n",
    "            (dir_path.path, dir_path.name, dir_path.size, dir_path.modificationTime)\n",
    "            if dir_path.isFile():\n",
    "                if (\n",
    "                    (\n",
    "                        dir_path.name.startswith(\"caregiverevent\")\n",
    "                        and dir_path.name.endswith(\".csv\")\n",
    "                        and dir_path.modificationTime > self.cg_unix_start_time\n",
    "                    )\n",
    "                    or (\n",
    "                        dir_path.name.startswith(\"CPIDODIEX_\")\n",
    "                        and dir_path.name.endswith(\".csv\")\n",
    "                        and \"NEW\" not in dir_path.name\n",
    "                        and dir_path.modificationTime > self.others_unix_start_time\n",
    "                    )\n",
    "                    or (\n",
    "                        dir_path.name.startswith(\"WRTS\")\n",
    "                        and dir_path.name.endswith(\".txt\")\n",
    "                        and dir_path.modificationTime > self.others_unix_start_time\n",
    "                    )\n",
    "                    or (\n",
    "                        dir_path.path.startswith(\n",
    "                            \"dbfs:/mnt/ci-vba-edw-2/DeltaTables/DW_ADHOC_RECURR.DOD_PATRONAGE_SCD_P\"\n",
    "                        )\n",
    "                        and dir_path.name.endswith(\"parquet\")\n",
    "                        and dir_path.modificationTime > self.others_unix_start_time\n",
    "\n",
    "                    )\n",
    "                ):\n",
    "                    yield dir_path\n",
    "            elif dir_path.isDir() and self.raw_file_folders != dir_path.path:\n",
    "                yield from self.get_all_files(dir_path.path, self.cg_unix_start_time, self.others_unix_start_time)\n",
    "\n",
    "    def collect_data_source(self):\n",
    "        \"\"\"\n",
    "        Collects files from directories, filters them based on modification time, and returns a DataFrame.\n",
    "        Returns:\n",
    "        DataFrame: PySpark DataFrame with files ready to process.\n",
    "        \"\"\"\n",
    "        raw_file_folders = self.source_directories()\n",
    "\n",
    "        cg_unix_start_time = (\n",
    "            int(time.mktime(datetime.strptime(self.cg_start_datetime, '%Y-%m-%d %H:%M:%S').timetuple())) * 1000\n",
    "        )\n",
    "        others_unix_start_time = (\n",
    "            int(time.mktime(datetime.strptime(self.others_start_datetime, '%Y-%m-%d %H:%M:%S').timetuple())) * 1000\n",
    "        )\n",
    "        query = f\"\"\"\n",
    "                    SELECT COALESCE(MAX(SDP_Event_Created_Timestamp), TIMESTAMP('{others_unix_start_time}')) AS max_date \n",
    "                    FROM {self.patronage_tablename}\n",
    "                    \"\"\"\n",
    "\n",
    "        max_processed_date = (\n",
    "            datetime.fromtimestamp(others_unix_start_time / 1000)\n",
    "            if self.no_of_files == 1\n",
    "            else spark.sql(query).collect()[0][0]\n",
    "        )\n",
    "\n",
    "        now = datetime.now()\n",
    "        yesterday_end_time = datetime(now.year, now.month, now.day) - timedelta(\n",
    "            hours=4\n",
    "        )  # Need to adjust to blob storage time\n",
    "        yesterday_end_time_ts = int(yesterday_end_time.timestamp() * 1000)\n",
    "        master_file_list = [\n",
    "            file\n",
    "            for folder in raw_file_folders\n",
    "            for file in self.get_all_files(folder, cg_unix_start_time, others_unix_start_time)\n",
    "        ]\n",
    "        file_list_df = spark.createDataFrame(master_file_list, file_list_schema)\n",
    "        filtered_file_list_df = (\n",
    "            file_list_df.withColumn(\n",
    "                \"dateTime\", to_timestamp(col(\"modificationTime\") / 1000)\n",
    "            )\n",
    "            # .filter(col(\"modificationTime\") > others_unix_start_time)\n",
    "            .filter(\n",
    "                (col(\"dateTime\") > max_processed_date)\n",
    "                & (col(\"modificationTime\") <= yesterday_end_time_ts)\n",
    "            )\n",
    "        )\n",
    "        # filtered_file_list_df.display()\n",
    "        parquet_PT_flag = (\n",
    "            filtered_file_list_df.filter(\n",
    "                filtered_file_list_df[\"path\"].contains(\"parquet\")\n",
    "            )\n",
    "            .orderBy(desc(col(\"modificationTime\")))\n",
    "            .limit(1)\n",
    "        )\n",
    "        # parquet_PT_flag is a reference that the delta table has been updated with fresh data for PT_Indicator.\n",
    "        all_other_files = filtered_file_list_df.filter(\n",
    "            ~filtered_file_list_df[\"path\"].contains(\"parquet\")\n",
    "        )\n",
    "        files_to_process_now = all_other_files.unionAll(parquet_PT_flag)\n",
    "        print(\n",
    "            f\"Upserting all files landed between date: {max_processed_date} and date: {yesterday_end_time}\"\n",
    "        )\n",
    "\n",
    "        if files_to_process_now.count() > 0:\n",
    "            return files_to_process_now.orderBy(col(\"modificationTime\"))\n",
    "        else:\n",
    "            dbutils.notebook.exit(\n",
    "                \"Notebook exited because there are no files to upsert.\"\n",
    "            )\n",
    "\n",
    "    def initialize_caregivers(self):\n",
    "        new_cg_df = spark.read.csv(\n",
    "            self.initial_cg_file,\n",
    "            header=True,\n",
    "            inferSchema=True,\n",
    "        )\n",
    "        transformed_cg_df = new_cg_df.select(\n",
    "            substring(\"ICN\", 1, 10).alias(\"ICN\"),\n",
    "            \"Applicant_Type\",\n",
    "            \"Caregiver_Status\",\n",
    "            date_format(\"Status_Begin_Date\", \"yyyyMMdd\").alias(\"Status_Begin_Date\"),\n",
    "            date_format(\"Status_Termination_Date\", \"yyyyMMdd\").alias(\n",
    "                \"Status_Termination_Date\"\n",
    "            ),\n",
    "            substring(\"Veteran_ICN\", 1, 10).alias(\"Veteran_ICN\"),\n",
    "        )\n",
    "        edipi_df = (\n",
    "            broadcast(transformed_cg_df)\n",
    "            .join(self.icn_relationship, [\"ICN\"], \"left\")\n",
    "            .withColumn(\n",
    "                \"filename\",\n",
    "                lit(self.initial_cg_file),\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"SDP_Event_Created_Timestamp\",\n",
    "                lit(self.cg_start_datetime).cast(TimestampType()),\n",
    "            )  # lit('2024-12-18T23:59:59.000+00:00')\n",
    "            .withColumn(\"Individual_Unemployability\", lit(None).cast(StringType()))\n",
    "            .withColumn(\"PT_Indicator\", lit(None).cast(StringType()))\n",
    "            .withColumn(\n",
    "                \"SC_Combined_Disability_Percentage\", lit(None).cast(StringType())\n",
    "            )\n",
    "            .withColumn(\"RecordStatus\", lit(True).cast(BooleanType()))\n",
    "            .withColumn(\"RecordLastUpdated\", lit(None).cast(DateType()))\n",
    "            .withColumn(\"Status_Last_Update\", lit(None).cast(StringType()))\n",
    "            .withColumn(\"sentToDoD\", lit(False).cast(BooleanType()))\n",
    "            .withColumn(\"Batch_CD\", lit(\"CG\").cast(StringType()))\n",
    "        )\n",
    "        return edipi_df\n",
    "\n",
    "    def process_updates(self, edipi_df, file_type):\n",
    "        \"\"\"\n",
    "        Upserts the input dataframe depending on input file type ('CG', 'PAI or 'SCD').\n",
    "        Uses Slowly Changing Dimensions type 2 logic that stores records that have been updated.\n",
    "        Parameters: Pyspark dataframe and file type\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        # Optimize the table based on the event timestamp\n",
    "        # spark.sql(\n",
    "        #     \"OPTIMIZE mypatronage_test111 ZORDER BY (SDP_Event_Created_Timestamp)\"\n",
    "        # )\n",
    "\n",
    "        # Load the target Delta table\n",
    "        targetTable = DeltaTable.forPath(\n",
    "            spark, self.fullname\n",
    "        )\n",
    "        targetDF = targetTable.toDF().filter(\n",
    "            (col(\"Batch_CD\") == file_type) & (col(\"RecordStatus\") == True)\n",
    "        )\n",
    "        # Rename columns in targetDF for clarity\n",
    "        targetDF = targetDF.select(\n",
    "            [col(c).alias(f\"target_{c}\") for c in targetDF.columns]\n",
    "        )\n",
    "\n",
    "        # Perform the join based on file type\n",
    "        if file_type in [\"CG\", \"SCD\"]:\n",
    "            joinDF = broadcast(edipi_df).join(\n",
    "                targetDF, join_conditions[file_type], \"leftouter\"\n",
    "            )\n",
    "\n",
    "            # Handling logic for SCD file type\n",
    "            if file_type == \"SCD\":\n",
    "                joinDF = (\n",
    "                    joinDF.withColumn(\"Status_Last_Update\", col(\"DSBL_DTR_DT\"))\n",
    "                    .withColumn(\n",
    "                        \"Status_Begin_Date\",\n",
    "                        coalesce(col(\"target_Status_Begin_Date\"), col(\"DSBL_DTR_DT\")),\n",
    "                    )\n",
    "                    .withColumn(\n",
    "                        \"PT_Indicator\",\n",
    "                        coalesce(joinDF[\"target_PT_Indicator\"], lit(\"N\")),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # Filter records that have changes based on delta condition\n",
    "            filterDF = joinDF.filter(delta_conditions[file_type])\n",
    "\n",
    "            # Handle dummy records with null MERGEKEY for unmatched records\n",
    "            mergeDF = filterDF.withColumn(\"MERGEKEY\", concat_column[file_type])\n",
    "            dummyDF = filterDF.filter(col(\"target_ICN\").isNotNull()).withColumn(\n",
    "                \"MERGEKEY\", lit(None)\n",
    "            )\n",
    "\n",
    "            # Union the filtered and dummy DataFrames\n",
    "            upsert_df = mergeDF.union(dummyDF)\n",
    "\n",
    "        if file_type == \"PAI\":\n",
    "            upsert_df = edipi_df\n",
    "\n",
    "        change_conditions = []\n",
    "        for source_col, target_col in columns_to_track[file_type]:\n",
    "            change_condition = when(\n",
    "                xxhash64(coalesce(col(source_col), lit(\"Null\")))\n",
    "                != xxhash64(coalesce(col(target_col), lit(\"Null\"))),\n",
    "                concat_ws(\n",
    "                    \" \",\n",
    "                    lit(source_col),\n",
    "                    lit(\"old value:\"),\n",
    "                    coalesce(col(target_col), lit(\"Null\")),\n",
    "                    lit(\"changed to new value:\"),\n",
    "                    coalesce(col(source_col), lit(\"Null\")),\n",
    "                ),\n",
    "            ).otherwise(lit(\"\"))\n",
    "            change_conditions.append(change_condition)\n",
    "        new_record_condition = when(\n",
    "            col(\"target_icn\").isNull(), lit(\"New Record\")\n",
    "        ).otherwise(lit(\"Updated Record\"))\n",
    "        upsert_df = upsert_df.withColumn(\"RecordChangeStatus\", new_record_condition)\n",
    "        if len(change_conditions) > 0:\n",
    "            change_log_col = concat_ws(\n",
    "                \" \", *[coalesce(cond, lit(\"\")) for cond in change_conditions]\n",
    "            )\n",
    "        else:\n",
    "            change_log_col = lit(\"\")\n",
    "        upsert_df = upsert_df.withColumn(\"change_log\", change_log_col)\n",
    "        if file_type == \"PAI\":\n",
    "            file_type = \"SCD\"\n",
    "        # Perform the merge operation\n",
    "        targetTable.alias(\"target\").merge(\n",
    "            upsert_df.alias(\"source\"), merge_conditions[file_type]\n",
    "        ).whenMatchedUpdate(\n",
    "            set={\n",
    "                \"RecordStatus\": \"False\",\n",
    "                \"RecordLastUpdated\": \"source.SDP_Event_Created_Timestamp\",\n",
    "                \"sentToDoD\": \"true\",\n",
    "                \"RecordChangeStatus\": lit(\"Expired Record\"),\n",
    "            }\n",
    "        ).whenNotMatchedInsert(\n",
    "            values={\n",
    "                \"edipi\": \"source.edipi\",\n",
    "                \"ICN\": \"source.ICN\",\n",
    "                \"Veteran_ICN\": \"source.Veteran_ICN\",\n",
    "                \"Applicant_Type\": \"source.Applicant_Type\",\n",
    "                \"Caregiver_Status\": \"source.Caregiver_Status\",\n",
    "                \"participant_id\": \"source.participant_id\",\n",
    "                \"Batch_CD\": \"source.Batch_CD\",\n",
    "                \"SC_Combined_Disability_Percentage\": \"source.SC_Combined_Disability_Percentage\",\n",
    "                \"PT_Indicator\": \"source.PT_Indicator\",\n",
    "                \"Individual_Unemployability\": \"source.Individual_Unemployability\",\n",
    "                \"Status_Begin_Date\": \"source.Status_Begin_Date\",\n",
    "                \"Status_Last_Update\": \"source.Status_Last_Update\",\n",
    "                \"Status_Termination_Date\": \"source.Status_Termination_Date\",\n",
    "                \"SDP_Event_Created_Timestamp\": \"source.SDP_Event_Created_Timestamp\",\n",
    "                \"RecordStatus\": \"true\",\n",
    "                \"RecordLastUpdated\": \"source.RecordLastUpdated\",\n",
    "                \"filename\": \"source.filename\",\n",
    "                \"sentToDoD\": \"false\",\n",
    "                \"change_log\": \"source.change_log\",\n",
    "                \"RecordChangeStatus\": \"source.RecordChangeStatus\",\n",
    "            }\n",
    "        ).execute()\n",
    "\n",
    "    def prepare_caregivers_data(self, cg_csv_files):\n",
    "        \"\"\"\n",
    "        Filters caregivers filenames from input dataframe, aggregates data and returns dataframe\n",
    "        Parameters: Pyspark dataframe with all filenames and metadata that are not processed (upsert)\n",
    "        Returns: Dataframe: Dataframe with required column names and edipi of a caregiver ready for upsert\n",
    "        \"\"\"\n",
    "        print(\n",
    "            f\"Upserting records from {cg_csv_files.count()} caregivers aggregated files\"\n",
    "        )\n",
    "        Window_Spec = Window.partitionBy(\n",
    "            \"ICN\", \"Veteran_ICN\", \"Applicant_Type\"\n",
    "        ).orderBy(desc(\"Event_Created_Date\"))\n",
    "\n",
    "        cg_csv_files_to_process = cg_csv_files.select(collect_list(\"path\")).first()[0]\n",
    "        cg_df = (\n",
    "            spark.read.schema(new_cg_schema)\n",
    "            .csv(cg_csv_files_to_process, header=True, inferSchema=False)\n",
    "            .selectExpr(\"*\", \"_metadata.file_name as filename\", \"_metadata.file_modification_time as SDP_Event_Created_Timestamp \" )\n",
    "            # .withColumn(\"filename\", col(\"_metadata.file_path\"))\n",
    "            # .withColumn(\n",
    "            #     \"SDP_Event_Created_Timestamp\",\n",
    "            #     (col(\"_metadata.file_modification_time\")).cast(TimestampType()),\n",
    "            # )\n",
    "        )\n",
    "        combined_cg_df = (\n",
    "            cg_df.select(\n",
    "                substring(\"Caregiver_ICN__c\", 1, 10).alias(\"ICN\"),\n",
    "                substring(\"Veteran_ICN__c\", 1, 10).alias(\"Veteran_ICN\"),\n",
    "                date_format(\"Benefits_End_Date__c\", \"yyyyMMdd\")\n",
    "                .alias(\"Status_Termination_Date\")\n",
    "                .cast(StringType()),\n",
    "                col(\"Applicant_Type__c\").alias(\"Applicant_Type\"),\n",
    "                col(\"Caregiver_Status__c\").alias(\"Caregiver_Status\"),\n",
    "                date_format(\"Dispositioned_Date__c\", \"yyyyMMdd\")\n",
    "                .alias(\"Status_Begin_Date\")\n",
    "                .cast(StringType()),\n",
    "                col(\"CreatedDate\").cast(\"timestamp\").alias(\"Event_Created_Date\"),\n",
    "                \"filename\",\n",
    "                \"SDP_Event_Created_Timestamp\",\n",
    "            )\n",
    "        ).filter(col(\"Caregiver_ICN__c\").isNotNull())\n",
    "        edipi_df = (\n",
    "            broadcast(combined_cg_df)\n",
    "            .join(self.icn_relationship, [\"ICN\"], \"left\")\n",
    "            .withColumn(\"Individual_Unemployability\", lit(None).cast(StringType()))\n",
    "            .withColumn(\"PT_Indicator\", lit(None).cast(StringType()))\n",
    "            .withColumn(\n",
    "                \"SC_Combined_Disability_Percentage\", lit(None).cast(StringType())\n",
    "            )\n",
    "            .withColumn(\"RecordStatus\", lit(True).cast(BooleanType()))\n",
    "            .withColumn(\"RecordLastUpdated\", lit(None).cast(DateType()))\n",
    "            .withColumn(\"Status_Last_Update\", lit(None).cast(StringType()))\n",
    "            .withColumn(\"sentToDoD\", lit(False).cast(BooleanType()))\n",
    "            .withColumn(\"Batch_CD\", lit(\"CG\").cast(StringType()))\n",
    "            .withColumn(\"rank\", rank().over(Window_Spec))\n",
    "            .filter(col(\"rank\") == 1)\n",
    "            .dropDuplicates()\n",
    "            .drop(\"rank\", \"va_profile_id\", \"record_updated_date\")\n",
    "        ).orderBy(col(\"Event_Created_Date\"))\n",
    "        return edipi_df\n",
    "\n",
    "    def prepare_scd_data(self, row):\n",
    "        \"\"\"\n",
    "        Prepares SCD data from the input row. This is the disability % data.\n",
    "        Parameters: Row of data from pyspark dataframe with filenames and metadata that are not processed (upsert)\n",
    "        Returns: Dataframe: Dataframe with required column names and edipi of a Veteran ready for upsert\n",
    "        \"\"\"\n",
    "\n",
    "        file_name = row.path\n",
    "\n",
    "        print(f\"Upserting records from {file_name}\")\n",
    "\n",
    "        if len(spark.read.csv(file_name).columns) != 3:\n",
    "            schema = scd_schema\n",
    "        else:\n",
    "            schema = scd_schema1\n",
    "        scd_updates_df = (\n",
    "            spark.read.csv(file_name, schema=schema, header=True, inferSchema=False)\n",
    "            .selectExpr(\n",
    "                \"PTCPNT_ID as participant_id\", \"CMBNED_DEGREE_DSBLTY\", \"DSBL_DTR_DT\", \"_metadata.file_name as filename\", \"_metadata.file_modification_time as SDP_Event_Created_Timestamp \"\n",
    "            )\n",
    "            # .withColumn(\"filename\", col(\"_metadata.file_path\"))\n",
    "            # .withColumn(\n",
    "            #     \"SDP_Event_Created_Timestamp\",\n",
    "            #     (col(\"_metadata.file_modification_time\")).cast(TimestampType()),\n",
    "            # )\n",
    "            .withColumn(\"sentToDoD\", lit(False).cast(BooleanType()))\n",
    "            .withColumn(\n",
    "                \"SC_Combined_Disability_Percentage\",\n",
    "                lpad(\n",
    "                    coalesce(\n",
    "                        when(col(\"CMBNED_DEGREE_DSBLTY\") == \"\", lit(\"000\")).otherwise(\n",
    "                            col(\"CMBNED_DEGREE_DSBLTY\")\n",
    "                        )\n",
    "                    ),\n",
    "                    3,\n",
    "                    \"0\",\n",
    "                ),\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"DSBL_DTR_DT\",\n",
    "                when(col(\"DSBL_DTR_DT\") == \"\", None).otherwise(\n",
    "                    date_format(to_date(col(\"DSBL_DTR_DT\"), \"MMddyyyy\"), \"yyyyMMdd\")\n",
    "                ),\n",
    "            )\n",
    "        ).filter(col(\"DSBL_DTR_DT\").isNotNull())\n",
    "\n",
    "        Window_Spec = Window.partitionBy(scd_updates_df[\"participant_id\"]).orderBy(\n",
    "            desc(\"DSBL_DTR_DT\"), desc(\"SC_Combined_Disability_Percentage\")\n",
    "        )\n",
    "        edipi_df = (\n",
    "            broadcast(scd_updates_df)\n",
    "            .join(self.icn_relationship, [\"participant_id\"], \"left\")\n",
    "            .withColumn(\"rank\", rank().over(Window_Spec))\n",
    "            .withColumn(\"Veteran_ICN\", lit(None).cast(StringType()))\n",
    "            .withColumn(\"Applicant_Type\", lit(None).cast(StringType()))\n",
    "            .withColumn(\"Caregiver_Status\", lit(None).cast(StringType()))\n",
    "            .withColumn(\"Individual_Unemployability\", lit(None).cast(StringType()))\n",
    "            .withColumn(\"Status_Termination_Date\", lit(None).cast(StringType()))\n",
    "            .withColumn(\"RecordLastUpdated\", lit(None).cast(DateType()))\n",
    "            .withColumn(\"Batch_CD\", lit(\"SCD\"))\n",
    "            .withColumn(\"RecordStatus\", lit(True).cast(BooleanType()))\n",
    "            .filter(col(\"rank\") == 1)\n",
    "            .filter(col(\"ICN\").isNotNull())\n",
    "            .dropDuplicates()\n",
    "            .drop(\"rank\", \"va_profile_id\", \"record_updated_date\")\n",
    "        )\n",
    "\n",
    "        return edipi_df\n",
    "\n",
    "    def update_pai_data(self, row, source_type):\n",
    "        \"\"\"\n",
    "        Prepares PT Indicator from the input row, transforms and updates a Veteran's PT_Indicator column in delta table.\n",
    "        Parameters: Row of data from pyspark dataframe with metadata that are not processed (upsert),\n",
    "                source_type is 'text' means its a static file (old source)\n",
    "                source_type is 'table' means its a delta table (new source)\n",
    "        Returns: Dataframe: Dataframe with required column names ready for upsert\n",
    "        \"\"\"\n",
    "        # print(source_type)\n",
    "        if source_type == \"text\":\n",
    "            file_name = row.path\n",
    "            file_creation_dateTime = row.dateTime\n",
    "            raw_pai_df = (\n",
    "                spark.read.csv(file_name, header=True, inferSchema=True)\n",
    "                .selectExpr(\"*\", \"_metadata.file_name as filename\", \"_metadata.file_modification_time as SDP_Event_Created_Timestamp \" )\n",
    "                # .withColumn(\n",
    "                #     \"SDP_Event_Created_Timestamp\",\n",
    "                #     (col(\"_metadata.file_modification_time\")).cast(TimestampType()),\n",
    "                # )\n",
    "                # .withColumn(\"filename\", col(\"_metadata.file_path\"))\n",
    "            )\n",
    "        elif source_type == \"table\":\n",
    "            file_creation_dateTime = row.dateTime\n",
    "            file_name = f\"Updated from PA&I delta table on {file_creation_dateTime}\"\n",
    "            raw_pai_df = spark.read.format(\"delta\").load(self.pt_new_source)\n",
    "\n",
    "        print(f\"Updating PT Indicator\")\n",
    "\n",
    "        targetTable = DeltaTable.forPath(\n",
    "            spark, self.fullname\n",
    "        )\n",
    "        targetDF = (\n",
    "            targetTable.toDF()\n",
    "            .filter(\"Batch_CD == 'SCD'\")\n",
    "            .filter(\"RecordStatus=='True'\")\n",
    "        )\n",
    "        targetDF = targetDF.select(\n",
    "            [col(c).alias(f\"target_{c}\") for c in targetDF.columns]\n",
    "        )\n",
    "\n",
    "        pai_df = raw_pai_df.selectExpr(\n",
    "            \"PTCPNT_VET_ID as participant_id\", \"PT_35_FLAG as source_PT_Indicator\"\n",
    "        )\n",
    "        existing_pai_data = f\"\"\"SELECT participant_id, PT_Indicator from {self.patronage_tablename} where RecordStatus is True and Batch_CD = 'SCD' \"\"\"\n",
    "        existing_pai_data_df = spark.sql(existing_pai_data)\n",
    "\n",
    "        joinDF = (\n",
    "            pai_df.join(\n",
    "                broadcast(targetDF),\n",
    "                pai_df[\"participant_id\"] == targetDF[\"target_participant_id\"],\n",
    "                \"left\",\n",
    "            )\n",
    "            .filter(targetDF[\"target_PT_Indicator\"] == \"N\")\n",
    "            .withColumn(\"filename\", lit(file_name))\n",
    "            .withColumn(\"SDP_Event_Created_Timestamp\", lit(file_creation_dateTime))\n",
    "        )\n",
    "\n",
    "        filterDF = joinDF.filter(\n",
    "            xxhash64(joinDF.source_PT_Indicator) != xxhash64(joinDF.target_PT_Indicator)\n",
    "        )\n",
    "\n",
    "        mergeDF = filterDF.withColumn(\"MERGEKEY\", filterDF.target_ICN)\n",
    "\n",
    "        dummyDF = filterDF.filter(\"target_ICN is not null\").withColumn(\n",
    "            \"MERGEKEY\", lit(None)\n",
    "        )\n",
    "\n",
    "        paiDF = mergeDF.union(dummyDF)\n",
    "        edipi_df = (\n",
    "            paiDF.selectExpr(\n",
    "                \"target_edipi as edipi\",\n",
    "                \"participant_id\",\n",
    "                \"target_ICN\",\n",
    "                \"MERGEKEY\",\n",
    "                \"target_SC_Combined_Disability_Percentage as SC_Combined_Disability_Percentage\",\n",
    "                \"target_Status_Begin_Date as Status_Begin_Date\",\n",
    "                \"target_Status_Last_Update as Status_Last_Update\",\n",
    "                \"SDP_Event_Created_Timestamp\",\n",
    "                \"filename\",\n",
    "                \"source_PT_Indicator\",\n",
    "                \"target_PT_Indicator\",\n",
    "            )\n",
    "            .withColumn(\"ICN\", lit(col(\"target_ICN\")))\n",
    "            .withColumn(\"Veteran_ICN\", lit(None))\n",
    "            .withColumn(\"Applicant_Type\", lit(None))\n",
    "            .withColumn(\"Caregiver_Status\", lit(None))\n",
    "            .withColumn(\"Batch_CD\", lit(\"SCD\"))\n",
    "            .withColumn(\"PT_Indicator\", coalesce(col(\"source_PT_Indicator\"), lit(\"N\")))\n",
    "            .withColumn(\"Individual_Unemployability\", lit(None))\n",
    "            .withColumn(\"Status_Termination_Date\", lit(None))\n",
    "            .withColumn(\"RecordLastUpdated\", lit(None))\n",
    "        )\n",
    "        return edipi_df\n",
    "\n",
    "    def process_files(self, files_to_process_now):\n",
    "        \"\"\"\n",
    "        Segregates files based on filename and calls required function to process these files\n",
    "        Parameters: Pyspark dataframe with filenames and metadata that are not processed\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        cg_csv_files = files_to_process_now.filter(\n",
    "            files_to_process_now[\"path\"].contains(\"caregiverevent\")\n",
    "        )\n",
    "        edipi_df = self.prepare_caregivers_data(cg_csv_files)\n",
    "        self.process_updates(edipi_df, \"CG\")\n",
    "        # display(cg_csv_files)\n",
    "\n",
    "        other_files = files_to_process_now.filter(\n",
    "            ~files_to_process_now[\"path\"].contains(\"caregiverevent\")\n",
    "        )\n",
    "        other_rows = other_files.collect()\n",
    "\n",
    "        for row in other_rows:\n",
    "            filename = row.path\n",
    "            if \"CPIDODIEX\" in filename:\n",
    "                # print(f\"Now processing: {row.path}\")\n",
    "                edipi_df = self.prepare_scd_data(row)\n",
    "                self.process_updates(edipi_df, \"SCD\")\n",
    "            elif \"WRTS\" in filename:\n",
    "                # print(f\"Now processing: {row.path}\")\n",
    "                edipi_df = self.update_pai_data(row, \"text\")\n",
    "                self.process_updates(edipi_df, \"PAI\")\n",
    "            elif \"parquet\" in filename:\n",
    "                # print(f\"Now processing: {row.path}\")\n",
    "                edipi_df = self.update_pai_data(row, \"table\")\n",
    "                self.process_updates(edipi_df, \"PAI\")\n",
    "            else:\n",
    "                pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5e4c0f21-05ae-46ca-92d8-879322421916",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Instantiating FileProcessor Class"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    file_processor = FileProcessor()\n",
    "\n",
    "    if file_processor.no_of_files == 0:\n",
    "        print(f\"Loading Caregivers Seed File into the {patronage_tablename} \")\n",
    "        seed_df = file_processor.initialize_caregivers()\n",
    "        file_processor.process_updates(seed_df, \"CG\")\n",
    "        print(\"Loading required files for upsert\")\n",
    "        files_to_process_now = file_processor.collect_data_source()\n",
    "        display(files_to_process_now)\n",
    "        print(f\"Total files to process in this run: {files_to_process_now.count()}...\")\n",
    "        file_processor.process_files(files_to_process_now)\n",
    "    else:\n",
    "        print(\"Loading required files for upsert\")\n",
    "        files_to_process_now = file_processor.collect_data_source()\n",
    "        display(files_to_process_now)\n",
    "        print(f\"Total files to process in this run: {files_to_process_now.count()}...\")\n",
    "        file_processor.process_files(files_to_process_now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7db3bfe-4b0d-454d-b8c2-5ae053bf1e51",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Main method to run the pipeline"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00561823-69c0-47ac-8a31-b670cbd34e14",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Listing of unprocessed files"
    }
   },
   "outputs": [],
   "source": [
    "file_processor = FileProcessor()\n",
    "(file_processor.collect_data_source()).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "447bbf08-2630-4bf8-80d2-b32f74a7aeaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  version,\n",
    "  timestamp,\n",
    "  operation,\n",
    "  cast(operationMetrics.numTargetRowsInserted as INT) as TotalInsertedRecords,\n",
    "  cast(operationMetrics.numTargetRowsUpdated as INT) as UpdatedRecords,\n",
    "    (cast(operationMetrics.numTargetRowsInserted as INT) -  cast(operationMetrics.numTargetRowsUpdated as INT)) as NewRecords\n",
    "\n",
    "FROM\n",
    "  (describe history mypatronage_test111)\n",
    "-- where\n",
    "--   operation not in (\"CREATE TABLE\", \"OPTIMIZE\")\n",
    "order by\n",
    "  1 desc;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c42b06-c57f-48d9-8c98-6f683faeeb0a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Number of records per processed day"
    }
   },
   "outputs": [],
   "source": [
    "recordsPerDate = f\"\"\" select count(*) as NoOfRecords, SDP_Event_Created_Timestamp \n",
    "                      from {file_processor.patronage_tablename}\n",
    "                      group by all\n",
    "                      order by 2 desc\"\"\"\n",
    "\n",
    "spark.sql(recordsPerDate).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fdaa38c-a1e6-45c0-b4e6-363b710df5f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6958556235126661,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4,
    "widgetLayout": []
   },
   "notebookName": "Patronage V4",
   "widgets": {
    "CaregiverSource": {
     "currentValue": "/mnt/ci-carma/landing/",
     "nuid": "e3628b61-6186-4ded-9df7-0bb88308a434",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/mnt/ci-carma/landing/",
      "label": "Caregiver Source",
      "name": "CaregiverSource",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/mnt/ci-carma/landing/",
      "label": "Caregiver Source",
      "name": "CaregiverSource",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "DisabilityPercentSource": {
     "currentValue": "/mnt/ci-vadir-shared/",
     "nuid": "8702b903-c77d-42c5-adf6-6995364e2a25",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/mnt/ci-vadir-shared/",
      "label": "Disability Percent Source",
      "name": "DisabilityPercentSource",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/mnt/ci-vadir-shared/",
      "label": "Disability Percent Source",
      "name": "DisabilityPercentSource",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "NewPTIndicatorSource": {
     "currentValue": "/mnt/ci-vba-edw-2/DeltaTables/DW_ADHOC_RECURR.DOD_PATRONAGE_SCD_PT/",
     "nuid": "06213c06-bf2a-4560-b380-dfe4d0f47911",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/mnt/ci-vba-edw-2/DeltaTables/DW_ADHOC_RECURR.DOD_PATRONAGE_SCD_PT/",
      "label": "New PT Indicator Source",
      "name": "NewPTIndicatorSource",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/mnt/ci-vba-edw-2/DeltaTables/DW_ADHOC_RECURR.DOD_PATRONAGE_SCD_PT/",
      "label": "New PT Indicator Source",
      "name": "NewPTIndicatorSource",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "OldPTIndicatorSource": {
     "currentValue": "/mnt/ci-patronage/pai_landing/",
     "nuid": "755b43ce-7a51-47fa-8998-d612c2fc594b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/mnt/ci-patronage/pai_landing/",
      "label": "Old PT Indicator Source",
      "name": "OldPTIndicatorSource",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/mnt/ci-patronage/pai_landing/",
      "label": "Old PT Indicator Source",
      "name": "OldPTIndicatorSource",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "PathToIdentityCorrelationsDeltaTable": {
     "currentValue": "/mnt/ci-patronage/delta_tables/correlation_lookup",
     "nuid": "c686e62d-a912-44dc-91ff-02f870f988e5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/mnt/Patronage/identity_correlations",
      "label": "Path to Identity Correlations Delta Table",
      "name": "PathToIdentityCorrelationsDeltaTable",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/mnt/Patronage/identity_correlations",
      "label": "Path to Identity Correlations Delta Table",
      "name": "PathToIdentityCorrelationsDeltaTable",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "PathToInitialSeedCaregiversCSVFile": {
     "currentValue": "dbfs:/FileStore/All_Caregivers_InitialSeed_12182024_csv.csv",
     "nuid": "a3f7d8f7-00bc-4096-b3bc-7dc3cc3cbe36",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dbfs:/FileStore/All_Caregivers_InitialSeed_12182024_csv.csv",
      "label": "Path to initial seed Caregivers csv file",
      "name": "PathToInitialSeedCaregiversCSVFile",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dbfs:/FileStore/All_Caregivers_InitialSeed_12182024_csv.csv",
      "label": "Path to initial seed Caregivers csv file",
      "name": "PathToInitialSeedCaregiversCSVFile",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "PatronageDeltaTableName": {
     "currentValue": "mypatronage_test111",
     "nuid": "3a926262-b9a1-4232-9a7f-792f9c80951b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "mypatronage_test111",
      "label": "Patronage Delta Table Name",
      "name": "PatronageDeltaTableName",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "mypatronage_test111",
      "label": "Patronage Delta Table Name",
      "name": "PatronageDeltaTableName",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "PatronageDeltaTablePath": {
     "currentValue": "dbfs:/user/hive/warehouse/",
     "nuid": "30c984c7-b976-4121-8f30-0b32ff8c9673",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dbfs:/user/hive/warehouse/",
      "label": "Patronage Delta Table Path",
      "name": "PatronageDeltaTablePath",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dbfs:/user/hive/warehouse/",
      "label": "Patronage Delta Table Path",
      "name": "PatronageDeltaTablePath",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "StartDateToProcessCaregiversData": {
     "currentValue": "2024-12-18 23:59:59",
     "nuid": "c3da3789-e2e4-4eee-b046-b4be03a63007",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2024-12-18 23:59:59",
      "label": "Start Date to Process Caregivers Data",
      "name": "StartDateToProcessCaregiversData",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2024-12-18 23:59:59",
      "label": "Start Date to Process Caregivers Data",
      "name": "StartDateToProcessCaregiversData",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "StartDateToProcessSCDData": {
     "currentValue": "2025-04-01 00:00:00",
     "nuid": "68e8aa64-75ef-47e7-afb7-1dd4bee3f985",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2025-04-01 00:00:00",
      "label": "Start Date to Process SCD Data",
      "name": "StartDateToProcessSCDData",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2025-04-01 00:00:00",
      "label": "Start Date to Process SCD Data",
      "name": "StartDateToProcessSCDData",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "rebuild": {
     "currentValue": "False",
     "nuid": "9d77865f-9272-4d55-98ff-865a8bf1f2c6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "False",
      "label": "1 Rebuild Patronage",
      "name": "rebuild",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "True",
        "False"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "False",
      "label": "1 Rebuild Patronage",
      "name": "rebuild",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "True",
        "False"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
