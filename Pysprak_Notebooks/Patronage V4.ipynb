{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acf912f6-2546-497b-894c-dd372e0aa886",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Patronage V4 - Data Lineage and Column Mapping\n",
    "\n",
    "## Delta Table Schema Documentation\n",
    "\n",
    "This table documents the complete data lineage for all columns in the Patronage delta table, including source systems, original column names, and any transformations applied.\n",
    "\n",
    "| Delta Table Column Name | Source | Original Column Name | Description of Transformation | Sent to DMDC |\n",
    "|------------------------|--------|---------------------|------------------------------|---------------|\n",
    "| **edipi** | Identity Correlations Delta Table | edipi | Direct mapping from ICN correlation lookup | **Yes** |\n",
    "| **ICN** | Multiple Sources | Caregiver_ICN__c (CG)<br/>ICN (Identity Correlations)<br/>ICN (from participant_id lookup) | **CG Source**: Truncated to first 10 characters from Caregiver_ICN__c<br/>**SCD Source**: Mapped from participant_id via identity correlations<br/>**Seed File**: Truncated to first 10 characters | No |\n",
    "| **Veteran_ICN** | Caregiver Sources Only | Veteran_ICN__c | **CG Source**: Truncated to first 10 characters<br/>**SCD/PAI Sources**: Set to NULL | No |\n",
    "| **participant_id** | Multiple Sources | participant_id (Identity Correlations)<br/>PTCPNT_ID (SCD)<br/>PTCPNT_VET_ID (PAI) | **SCD Source**: Direct mapping from PTCPNT_ID<br/>**PAI Source**: Direct mapping from PTCPNT_VET_ID<br/>**CG Source**: Retrieved via ICN lookup from identity correlations | No |\n",
    "| **Batch_CD** | System Generated | N/A | **CG Records**: Hard-coded as \"CG\"<br/>**SCD Records**: Hard-coded as \"SCD\"<br/>**PAI Records**: Hard-coded as \"SCD\" (processed as SCD updates) | **Yes** |\n",
    "| **Applicant_Type** | Caregiver Sources Only | Applicant_Type__c | **CG Source**: Direct mapping<br/>**SCD/PAI Sources**: Set to NULL | No |\n",
    "| **Caregiver_Status** | Caregiver Sources Only | Caregiver_Status__c | **CG Source**: Direct mapping<br/>**SCD/PAI Sources**: Set to NULL | No |\n",
    "| **SC_Combined_Disability_Percentage** | SCD Sources Only | CMBNED_DEGREE_DSBLTY | **SCD Source**: Zero-padded to 3 digits, empty strings converted to \"000\"<br/>**CG/PAI Sources**: Set to NULL | **Yes** |\n",
    "| **PT_Indicator** | PAI Sources + Default | PT_35_FLAG (PAI)<br/>target_PT_Indicator (existing records) | **PAI Source**: Direct mapping from PT_35_FLAG<br/>**SCD Records**: Defaults to \"N\" for new records, preserves existing values<br/>**CG Records**: Set to NULL | **Yes** |\n",
    "| **Individual_Unemployability** | Not Currently Populated | N/A | Set to NULL for all sources (placeholder for future implementation) | **Yes** |\n",
    "| **Status_Begin_Date** | Multiple Sources | Dispositioned_Date__c (CG)<br/>DSBL_DTR_DT (SCD)<br/>target_Status_Begin_Date (existing) | **CG Source**: Date formatted from Dispositioned_Date__c to YYYYMMDD<br/>**SCD Source**: Uses existing Status_Begin_Date or DSBL_DTR_DT if new record<br/>**Date Format**: Converted from MMddyyyy to yyyyMMdd | **Yes** |\n",
    "| **Status_Last_Update** | Multiple Sources | DSBL_DTR_DT (SCD)<br/>N/A (CG) | **SCD Source**: Direct mapping from DSBL_DTR_DT<br/>**CG Source**: Set to NULL | **Yes** |\n",
    "| **Status_Termination_Date** | Caregiver Sources Only | Benefits_End_Date__c | **CG Source**: Date formatted from Benefits_End_Date__c to YYYYMMDD<br/>**SCD/PAI Sources**: Set to NULL | **Yes** |\n",
    "| **SDP_Event_Created_Timestamp** | File Metadata | _metadata.file_modification_time<br/>CreatedDate (seed) | **All File Sources**: Extracted from file modification timestamp<br/>**Seed File**: Uses configured start datetime<br/>**PAI Delta Table**: Uses current datetime | No |\n",
    "| **filename** | File Metadata + System | _metadata.file_name<br/>Path (seed)<br/>Generated (PAI) | **File Sources**: Extracted from file metadata<br/>**Seed File**: Full file path<br/>**PAI Delta Updates**: Generated description with timestamp | No |\n",
    "| **RecordLastUpdated** | System Generated | N/A | **New Records**: Set to NULL<br/>**Updated Records**: Set to SDP_Event_Created_Timestamp during merge | No |\n",
    "| **RecordStatus** | System Generated | N/A | **Active Records**: Set to TRUE<br/>**Expired Records**: Set to FALSE during SCD Type 2 updates | No |\n",
    "| **sentToDoD** | System Generated | N/A | **New Records**: Set to FALSE<br/>**Expired Records**: Set to TRUE during updates | No |\n",
    "| **change_log** | System Generated | N/A | **New Records**: \"New Record\"<br/>**Updated Records**: Detailed log of field changes with oldâ†’new values | No |\n",
    "| **RecordChangeStatus** | System Generated | N/A | **New Records**: \"New Record\"<br/>**Updated Records**: \"Updated Record\"<br/>**Expired Records**: \"Expired Record\" | No |\n",
    "\n",
    "## Data Source Details\n",
    "\n",
    "### Primary Data Sources:\n",
    "1. **Caregiver Events (CG)**: CARMA system CSV files (`caregiverevent*.csv`)\n",
    "2. **Service-Connected Disability (SCD)**: VA disability files (`CPIDODIEX_*.csv`)\n",
    "3. **PT Indicator Legacy (PAI)**: Text files (`WRTS*.txt`)\n",
    "4. **PT Indicator Modern (PAI)**: Delta table (`DW_ADHOC_RECURR.DOD_PATRONAGE_SCD_PT`)\n",
    "5. **Identity Correlations**: Delta table mapping ICNs to EDIPIs and participant IDs\n",
    "6. **Seed Data**: Initial caregiver population CSV file\n",
    "\n",
    "### Key Transformation Patterns:\n",
    "- **ICN Standardization**: All ICNs truncated to 10 characters for consistency\n",
    "- **Date Standardization**: All dates converted to YYYYMMDD string format\n",
    "- **Null Handling**: Explicit NULL assignment for irrelevant fields per source type\n",
    "- **Change Detection**: xxhash64 used for efficient change identification\n",
    "- **Deduplication**: Window functions ensure latest record per unique key combination\n",
    "- **Audit Trail**: Complete change tracking with before/after values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdbaa5ea-5d66-44e0-87c1-0a388e8cffc2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Importing Libraries"
    }
   },
   "outputs": [],
   "source": [
    "# /mnt/Patronage/identity_correlations\n",
    "from delta.tables import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime, timedelta, date\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fedf12bd-6dbf-44ca-bdd0-806a70e411fe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Widget to name Patronage delta table and the Path of the table"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"PatronageDeltaTableName\",\"mypat_test\", \"Patronage Delta Table Name\")\n",
    "dbutils.widgets.text(\"PatronageDeltaTablePath\",\"dbfs:/user/hive/warehouse/\", \"Patronage Delta Table Path\")\n",
    "patronage_tablename = dbutils.widgets.get(\"PatronageDeltaTableName\")\n",
    "patronage_table_location = dbutils.widgets.get(\"PatronageDeltaTablePath\")\n",
    "fullname = patronage_table_location + patronage_tablename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00183229-72b4-4446-b321-b4da15e923e9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Widget to Rebuild Patronage Delta Table"
    }
   },
   "outputs": [],
   "source": [
    "# Choose True to rebuild Patronage Delta Table\n",
    "dbutils.widgets.dropdown(\"rebuild\", \"False\", [\"True\", \"False\"], \"1 Rebuild Patronage\")\n",
    "boolean_value = dbutils.widgets.get(\"rebuild\")\n",
    "\n",
    "if boolean_value == 'True':\n",
    "    dropTableQuery = f\"\"\" DROP TABLE IF EXISTS {patronage_tablename} \"\"\"\n",
    "    spark.sql(dropTableQuery)\n",
    "    dbutils.fs.rm(fullname, True)\n",
    "else:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84f90464-c223-4d33-b519-f97256ae3459",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating Variables and Patronage delta table"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"CaregiverSource\", \"/mnt/ci-carma/landing/\", \"Caregiver Source\")\n",
    "dbutils.widgets.text(\"DisabilityPercentSource\", \"/mnt/ci-vadir-shared/\", \"Disability Percent Source\")\n",
    "dbutils.widgets.text(\"NewPTIndicatorSource\", \"/mnt/ci-vba-edw-2/DeltaTables/DW_ADHOC_RECURR.DOD_PATRONAGE_SCD_PT/\", \"New PT Indicator Source\")\n",
    "\n",
    "dbutils.widgets.text(\"StartDateToProcessCaregiversData\",\"2024-12-18 23:59:59\", \"Start Date to Process Caregivers Data\")\n",
    "dbutils.widgets.text(\"StartDateToProcessSCDData\",\"2025-06-01 00:00:00\", \"Start Date to Process SCD Data\")\n",
    "dbutils.widgets.text(\"PathToInitialSeedCaregiversCSVFile\", \"dbfs:/FileStore/All_Caregivers_InitialSeed_12182024_csv.csv\", \"Path to initial seed Caregivers csv file\")\n",
    "dbutils.widgets.text(\"PathToIdentityCorrelationsDeltaTable\", \"/mnt/Patronage/identity_correlations\", \"Path to Identity Correlations Delta Table\")\n",
    "initial_cg_file = dbutils.widgets.get(\"PathToInitialSeedCaregiversCSVFile\")\n",
    "cg_source = dbutils.widgets.get(\"CaregiverSource\")\n",
    "scd_source = dbutils.widgets.get(\"DisabilityPercentSource\")\n",
    "pt_new_source = dbutils.widgets.get(\"NewPTIndicatorSource\")\n",
    "\n",
    "if not DeltaTable.isDeltaTable(spark, fullname):\n",
    "        print(f\"Creating {patronage_tablename}, on location {fullname}\")\n",
    "        create_table_query = f\"\"\"CREATE TABLE IF NOT EXISTS {patronage_tablename} (edipi string, ICN string, Veteran_ICN string, participant_id string, Batch_CD string,  Applicant_Type string, Caregiver_Status string, SC_Combined_Disability_Percentage string, PT_Indicator string, Individual_Unemployability string, Status_Begin_Date string, Status_Last_Update string, Status_Termination_Date string, SDP_Event_Created_Timestamp timestamp, filename string, RecordLastUpdated date, RecordStatus boolean, sentToDoD boolean, change_log string, RecordChangeStatus string, DateSentToDoD date) PARTITIONED BY(Batch_CD, RecordStatus ) LOCATION '{fullname}' \"\"\"\n",
    "        spark.sql(create_table_query)\n",
    "        file_count_query = f\"SELECT COALESCE(COUNT(DISTINCT filename), 0) AS count FROM DELTA.`{fullname}`\"\n",
    "        no_of_files = spark.sql(file_count_query).collect()[0][0]\n",
    "        print(f\"Total files processed till last run: {no_of_files}\")\n",
    "else:\n",
    "        file_count_query = f\"SELECT COALESCE(COUNT(DISTINCT filename), 0) AS count FROM DELTA.`{fullname}`\"\n",
    "        no_of_files = spark.sql(file_count_query).collect()[0][0]\n",
    "        print(f\"Total files processed till last run: {no_of_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b843903b-a19c-4984-abfd-0f59fabb2d69",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Defining necessary schemas"
    }
   },
   "outputs": [],
   "source": [
    "# new_cg_schema = StructType(\n",
    "#     [\n",
    "#         StructField(\"Discharge_Revocation_Date__c\", StringType(), True),\n",
    "#         StructField(\"Caregiver_Status__c\", StringType(), True),\n",
    "#         StructField(\"CreatedById\", StringType(), True),\n",
    "#         StructField(\"Dispositioned_Date__c\", StringType(), True),\n",
    "#         StructField(\"CARMA_Case_ID__c\", StringType(), True),\n",
    "#         StructField(\"Applicant_Type__c\", StringType(), True),\n",
    "#         StructField(\"CreatedDate\", StringType(), True),\n",
    "#         StructField(\"Veteran_ICN__c\", StringType(), True),\n",
    "#         StructField(\"Benefits_End_Date__c\", StringType(), True),\n",
    "#         StructField(\"Caregiver_Id__c\", StringType(), True),\n",
    "#         StructField(\"CARMA_Case_Number__c\", StringType(), True),\n",
    "#         StructField(\"Caregiver_ICN__c\", StringType(), True),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# scd_schema = StructType(\n",
    "#     [\n",
    "#         StructField(\"PTCPNT_ID\", StringType()),\n",
    "#         StructField(\"FILE_NBR\", StringType()),\n",
    "#         StructField(\"LAST_NM\", StringType()),\n",
    "#         StructField(\"FIRST_NM\", StringType()),\n",
    "#         StructField(\"MIDDLE_NM\", StringType()),\n",
    "#         StructField(\"SUFFIX_NM\", StringType()),\n",
    "#         StructField(\"STA_NBR\", StringType()),\n",
    "#         StructField(\"BRANCH_OF_SERV\", StringType()),\n",
    "#         StructField(\"DATE_OF_BIRTH\", StringType()),\n",
    "#         StructField(\"DATE_OF_DEATH\", StringType()),\n",
    "#         StructField(\"VET_SSN_NBR\", StringType()),\n",
    "#         StructField(\"SVC_NBR\", StringType()),\n",
    "#         StructField(\"AMT_GROSS_OR_NET_AWARD\", IntegerType()),\n",
    "#         StructField(\"AMT_NET_AWARD\", IntegerType()),\n",
    "#         StructField(\"NET_AWARD_DATE\", StringType()),\n",
    "#         StructField(\"SPECL_LAW_IND\", IntegerType()),\n",
    "#         StructField(\"VET_SSN_VRFCTN_IND\", IntegerType()),\n",
    "#         StructField(\"WIDOW_SSN_VRFCTN_IND\", IntegerType()),\n",
    "#         StructField(\"PAYEE_SSN\", StringType()),\n",
    "#         StructField(\"ADDRS_ONE_TEXT\", StringType()),\n",
    "#         StructField(\"ADDRS_TWO_TEXT\", StringType()),\n",
    "#         StructField(\"ADDRS_THREE_TEXT\", StringType()),\n",
    "#         StructField(\"ADDRS_CITY_NM\", StringType()),\n",
    "#         StructField(\"ADDRS_ST_CD\", StringType()),\n",
    "#         StructField(\"ADDRS_ZIP_PREFIX_NBR\", IntegerType()),\n",
    "#         StructField(\"MIL_POST_OFFICE_TYP_CD\", StringType()),\n",
    "#         StructField(\"MIL_POSTAL_TYPE_CD\", StringType()),\n",
    "#         StructField(\"COUNTRY_TYPE_CODE\", IntegerType()),\n",
    "#         StructField(\"SUSPENSE_IND\", IntegerType()),\n",
    "#         StructField(\"PAYEE_NBR\", IntegerType()),\n",
    "#         StructField(\"EOD_DT\", StringType()),\n",
    "#         StructField(\"RAD_DT\", StringType()),\n",
    "#         StructField(\"ADDTNL_SVC_IND\", StringType()),\n",
    "#         StructField(\"ENTLMT_CD\", StringType()),\n",
    "#         StructField(\"DSCHRG_PAY_GRADE_NM\", StringType()),\n",
    "#         StructField(\"AMT_OF_OTHER_RETIREMENT\", IntegerType()),\n",
    "#         StructField(\"RSRVST_IND\", StringType()),\n",
    "#         StructField(\"NBR_DAYS_ACTIVE_RESRV\", IntegerType()),\n",
    "#         StructField(\"CMBNED_DEGREE_DSBLTY\", StringType()),\n",
    "#         StructField(\"DSBL_DTR_DT\", StringType()),\n",
    "#         StructField(\"DSBL_TYP_CD\", StringType()),\n",
    "#         StructField(\"VA_SPCL_PROV_CD\", IntegerType()),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "scd_schema1 = StructType(\n",
    "    [\n",
    "        StructField(\"PTCPNT_ID\", StringType()),\n",
    "        StructField(\"CMBNED_DEGREE_DSBLTY\", StringType()),\n",
    "        StructField(\"DSBL_DTR_DT\", StringType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# pai_schema = StructType(\n",
    "#     [\n",
    "#         StructField(\"EDI_PI\", StringType()),\n",
    "#         StructField(\"SSN_NBR\", StringType()),\n",
    "#         StructField(\"FILE_NBR\", StringType()),\n",
    "#         StructField(\"PTCPNT_VET_ID\", StringType()),\n",
    "#         StructField(\"LAST_NM\", StringType()),\n",
    "#         StructField(\"FIRST_NM\", StringType()),\n",
    "#         StructField(\"MIDDLE_NM\", StringType()),\n",
    "#         StructField(\"PT35_RATING_DT\", TimestampType()),\n",
    "#         StructField(\"PT35_PRMLGN_DT\", TimestampType()),\n",
    "#         StructField(\"PT35_EFFECTIVE_DATE\", TimestampType()),\n",
    "#         StructField(\"PT35_END_DATE\", TimestampType()),\n",
    "#         StructField(\"PT_35_FLAG\", StringType()),\n",
    "#         StructField(\"COMBND_DEGREE_PCT\", StringType()),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "file_list_schema = StructType(\n",
    "    [\n",
    "        StructField(\"path\", StringType()),\n",
    "        StructField(\"name\", StringType()),\n",
    "        StructField(\"size\", StringType()),\n",
    "        StructField(\"modificationTime\", StringType()),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9133237d-d871-4495-a16f-def76ab0fc44",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Defining various conditions  for Patronage data"
    }
   },
   "outputs": [],
   "source": [
    "# Define join conditions based on file type\n",
    "join_conditions = {\n",
    "    \"CG\": (\n",
    "        (col(\"ICN\") == col(\"target_ICN\"))\n",
    "        & (col(\"Veteran_ICN\") == col(\"target_Veteran_ICN\"))\n",
    "        & (col(\"Batch_CD\") == col(\"target_Batch_CD\"))\n",
    "        & (col(\"Applicant_Type\") == col(\"target_Applicant_Type\"))\n",
    "        & (col(\"target_RecordStatus\") == True)\n",
    "    ),\n",
    "    \"SCD\": (\n",
    "        (col(\"ICN\") == col(\"target_ICN\"))\n",
    "        & (col(\"target_RecordStatus\") == True)\n",
    "        & (col(\"Batch_CD\") == col(\"target_Batch_CD\"))\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Define delta condition based on file type\n",
    "delta_conditions = {\n",
    "    \"CG\": xxhash64(\n",
    "        col(\"Status_Begin_Date\"),\n",
    "        col(\"Status_Termination_Date\"),\n",
    "        col(\"Applicant_Type\"),\n",
    "        col(\"Caregiver_Status\")\n",
    "    ) != xxhash64(\n",
    "        col(\"target_Status_Begin_Date\"),\n",
    "        col(\"target_Status_Termination_Date\"),\n",
    "        col(\"target_Applicant_Type\"),\n",
    "        col(\"target_Caregiver_Status\")\n",
    "    ),\n",
    "    \"SCD\": xxhash64(\n",
    "        col(\"SC_Combined_Disability_Percentage\")\n",
    "    ) != xxhash64(col(\"target_SC_Combined_Disability_Percentage\")),\n",
    "}\n",
    "\n",
    "# Track changes in specified columns and create a change log\n",
    "columns_to_track = {\n",
    "    \"CG\": [\n",
    "        (\"Status_Begin_Date\", \"target_Status_Begin_Date\"),\n",
    "        (\"Status_Termination_Date\", \"target_Status_Termination_Date\"),\n",
    "        (\"Applicant_Type\", \"target_Applicant_Type\"),\n",
    "        (\"Caregiver_Status\", \"target_Caregiver_Status\")\n",
    "    ],\n",
    "    \"SCD\": [(\n",
    "        \"SC_Combined_Disability_Percentage\", \"target_SC_Combined_Disability_Percentage\"),\n",
    "    ],\n",
    "    \"PAI\": [\n",
    "        (\"PT_Indicator\", \"target_PT_Indicator\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Define merge conditions and batch_cd\n",
    "merge_conditions = {\n",
    "    \"CG\": \"concat(target.ICN, target.Veteran_ICN, target.Applicant_Type) = source.MERGEKEY and target.RecordStatus = True\",\n",
    "    \"SCD\": \"((target.ICN = source.MERGEKEY) and (target.Batch_CD = source.Batch_CD) and (target.RecordStatus = True))\"\n",
    "}\n",
    "\n",
    "# Define concat_column based on file type\n",
    "concat_column = {\n",
    "    \"CG\": concat(col(\"ICN\"), col(\"Veteran_ICN\"), col(\"Applicant_Type\")),\n",
    "    \"SCD\": col(\"ICN\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5436e1d-b857-46e9-a07f-5a0811b1d291",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating FileProcessor Class and other methods for Caregivers and SCD"
    }
   },
   "outputs": [],
   "source": [
    "class FileProcessor:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.initial_cg_file = initial_cg_file\n",
    "        self.cg_source = cg_source\n",
    "        self.scd_source = scd_source\n",
    "        self.pt_new_source = pt_new_source\n",
    "        self.patronage_tablename = patronage_tablename\n",
    "        self.patronage_table_location = patronage_table_location\n",
    "        self.fullname = fullname\n",
    "        self.no_of_files = no_of_files\n",
    "        self.cg_start_datetime = dbutils.widgets.get(\"StartDateToProcessCaregiversData\")\n",
    "        self.others_start_datetime = dbutils.widgets.get(\"StartDateToProcessSCDData\")\n",
    "        self.identity_correlations_path = dbutils.widgets.get(\"PathToIdentityCorrelationsDeltaTable\")\n",
    "        self.icn_relationship = (spark.read.format(\"delta\")\n",
    "                    .load(self.identity_correlations_path)\n",
    "                    .withColumnRenamed('MVIPersonICN', 'ICN')).persist()\n",
    "\n",
    "    def collect_data_source(self):\n",
    "        now = datetime.now()\n",
    "        yesterday_end_time = datetime(now.year, now.month, now.day) - timedelta(\n",
    "            hours=4\n",
    "        )  # Need to adjust to blob storage time\n",
    "        yesterday_end_time_ts = int(yesterday_end_time.timestamp() * 1000)\n",
    "        \n",
    "        scd_beginning_datetime = self.others_start_datetime\n",
    "        scd_beginning_datetime = datetime(now.year, 1, 1) # Beginning of the year\n",
    "\n",
    "        print(f\"Yesterday's End Time: {yesterday_end_time}, Equivalent unix time: {yesterday_end_time_ts}\")\n",
    "        print(f\"Number of files: {no_of_files}\")\n",
    "\n",
    "        readFiles_start_datetime = self.cg_start_datetime\n",
    "        unix_start_time = (\n",
    "            int(time.mktime(datetime.strptime(readFiles_start_datetime, '%Y-%m-%d %H:%M:%S').timetuple())) * 1000\n",
    "        )\n",
    "\n",
    "        file_count_query = f\"SELECT COALESCE(COUNT(DISTINCT filename), 0) AS count FROM mypat_test\"\n",
    "\n",
    "        query = f\"\"\" SELECT COALESCE(MAX(SDP_Event_Created_Timestamp), to_timestamp('{readFiles_start_datetime}')) AS max_date FROM mypat_test\n",
    "                \"\"\"\n",
    "        max_processed_date = spark.sql(query).collect()[0][0]\n",
    "        print(f\"Max Date Processed: {max_processed_date}\")\n",
    "        print(f\"scd_beginning_datetime: {scd_beginning_datetime}\")\n",
    "\n",
    "        all_scd_files = (spark.createDataFrame(dbutils.fs.ls(\"/mnt/ci-vadir-shared/\"))\n",
    "                        .filter(col(\"name\").startswith(\"CPIDODIEX_\") \n",
    "                                & col(\"name\").endswith(\".csv\") \n",
    "                                & ~(col(\"name\").contains(\"NEW\"))\n",
    "                        ).filter(to_timestamp(col(\"modificationTime\")/1000) > scd_beginning_datetime )\n",
    "        )\n",
    "\n",
    "        all_cg_files =  (spark.createDataFrame(dbutils.fs.ls(\"/mnt/ci-carma/landing/\"))\n",
    "                        .filter(col(\"name\").contains(\"caregiverevent\") \n",
    "                                & col(\"name\").endswith(\".csv\")\n",
    "                        ).filter(to_timestamp(col(\"modificationTime\")/1000) > readFiles_start_datetime )\n",
    "        )\n",
    "\n",
    "        all_pt_files = (spark.createDataFrame(dbutils.fs.ls(\"/mnt/ci-vba-edw-2/DeltaTables/DW_ADHOC_RECURR.DOD_PATRONAGE_SCD_PT/\"))\n",
    "                        .filter(to_timestamp(col(\"modificationTime\")/1000) > scd_beginning_datetime)\n",
    "                        .orderBy(desc(col(\"modificationTime\")))\n",
    "                        .limit(1)\n",
    "        )\n",
    "\n",
    "        combined_files = all_scd_files.unionAll(all_cg_files).unionAll(all_pt_files)\n",
    "\n",
    "        filtered_files = (combined_files\n",
    "                        .withColumn(\"dateTime\", to_timestamp(col(\"modificationTime\")/1000))\n",
    "                        .filter((col(\"dateTime\") > (max_processed_date )) \n",
    "                                & (col(\"modificationTime\") <= yesterday_end_time_ts)\n",
    "                        )\n",
    "                        .orderBy(col(\"modificationTime\").desc())\n",
    "        )\n",
    "        print(f\"files to process, {filtered_files.count()}\")\n",
    "\n",
    "        if filtered_files.count() > 0:\n",
    "            return filtered_files.orderBy(col(\"modificationTime\"))\n",
    "        else:\n",
    "            dbutils.notebook.exit(\"Notebook exited because no files to process\")\n",
    "\n",
    "    def initialize_caregivers(self):\n",
    "        new_cg_df = spark.read.csv(\n",
    "            self.initial_cg_file,\n",
    "            header=True,\n",
    "            inferSchema=True,\n",
    "        )\n",
    "        transformed_cg_df = new_cg_df.select(\n",
    "            substring(\"ICN\", 1, 10).alias(\"ICN\"),\n",
    "            \"Applicant_Type\",\n",
    "            \"Caregiver_Status\",\n",
    "            date_format(\"Status_Begin_Date\", \"yyyyMMdd\").alias(\"Status_Begin_Date\"),\n",
    "            date_format(\"Status_Termination_Date\", \"yyyyMMdd\").alias(\n",
    "                \"Status_Termination_Date\"\n",
    "            ),\n",
    "            substring(\"Veteran_ICN\", 1, 10).alias(\"Veteran_ICN\"),\n",
    "        )\n",
    "        edipi_df = (\n",
    "            broadcast(transformed_cg_df)\n",
    "            .join(self.icn_relationship, [\"ICN\"], \"left\")\n",
    "            .withColumn(\n",
    "                \"filename\",\n",
    "                lit(self.initial_cg_file),\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"SDP_Event_Created_Timestamp\",\n",
    "                lit(self.cg_start_datetime).cast(TimestampType()),\n",
    "            )  # lit('2024-12-18T23:59:59.000+00:00')\n",
    "            .withColumn(\"Individual_Unemployability\", lit(None).cast(StringType()))\n",
    "            .withColumn(\"PT_Indicator\", lit(None).cast(StringType()))\n",
    "            .withColumn(\n",
    "                \"SC_Combined_Disability_Percentage\", lit(None).cast(StringType())\n",
    "            )\n",
    "            .withColumn(\"RecordStatus\", lit(True).cast(BooleanType()))\n",
    "            .withColumn(\"RecordLastUpdated\", lit(None).cast(DateType()))\n",
    "            .withColumn(\"Status_Last_Update\", lit(None).cast(StringType()))\n",
    "            .withColumn(\"sentToDoD\", lit(False).cast(BooleanType()))\n",
    "            .withColumn(\"Batch_CD\", lit(\"CG\").cast(StringType()))\n",
    "        )\n",
    "        return edipi_df\n",
    "\n",
    "    def process_updates(self, edipi_df, file_type):\n",
    "        \"\"\"\n",
    "        Upserts the input dataframe depending on input file type ('CG', 'PAI or 'SCD').\n",
    "        Uses Slowly Changing Dimensions type 2 logic that stores records that have been updated.\n",
    "        Parameters: Pyspark dataframe and file type\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    " \n",
    "        # Load the target Delta table\n",
    "        targetTable = DeltaTable.forPath(\n",
    "            spark, self.fullname\n",
    "        )\n",
    "        targetDF = targetTable.toDF().filter(\n",
    "            (col(\"Batch_CD\") == file_type) & (col(\"RecordStatus\") == True)\n",
    "        )\n",
    "        # Rename columns in targetDF for clarity\n",
    "        targetDF = targetDF.select(\n",
    "            [col(c).alias(f\"target_{c}\") for c in targetDF.columns]\n",
    "        )\n",
    "\n",
    "        # Perform the join based on file type\n",
    "        if file_type in [\"CG\", \"SCD\"]:\n",
    "            joinDF = broadcast(edipi_df).join(\n",
    "                targetDF, join_conditions[file_type], \"leftouter\"\n",
    "            )\n",
    "\n",
    "            # Handling logic for SCD file type\n",
    "            if file_type == \"SCD\":\n",
    "                joinDF = (\n",
    "                    joinDF.withColumn(\"Status_Last_Update\", col(\"DSBL_DTR_DT\"))\n",
    "                    .withColumn(\n",
    "                        \"Status_Begin_Date\",\n",
    "                        coalesce(col(\"target_Status_Begin_Date\"), col(\"DSBL_DTR_DT\")),\n",
    "                    )\n",
    "                    .withColumn(\n",
    "                        \"PT_Indicator\",\n",
    "                        coalesce(joinDF[\"target_PT_Indicator\"], lit(\"N\")),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # Filter records that have changes based on delta condition\n",
    "            filterDF = joinDF.filter(delta_conditions[file_type])\n",
    "\n",
    "            # Handle dummy records with null MERGEKEY for unmatched records\n",
    "            mergeDF = filterDF.withColumn(\"MERGEKEY\", concat_column[file_type])\n",
    "            dummyDF = filterDF.filter(col(\"target_ICN\").isNotNull()).withColumn(\n",
    "                \"MERGEKEY\", lit(None)\n",
    "            )\n",
    "\n",
    "            # Union the filtered and dummy DataFrames\n",
    "            upsert_df = mergeDF.union(dummyDF)\n",
    "\n",
    "        if file_type == \"PAI\":\n",
    "            upsert_df = edipi_df\n",
    "\n",
    "        change_conditions = []\n",
    "        for source_col, target_col in columns_to_track[file_type]:\n",
    "            change_condition = when(\n",
    "                xxhash64(coalesce(col(source_col), lit(\"Null\")))\n",
    "                != xxhash64(coalesce(col(target_col), lit(\"Null\"))),\n",
    "                concat_ws(\n",
    "                    \" \",\n",
    "                    lit(source_col),\n",
    "                    lit(\"old value:\"),\n",
    "                    coalesce(col(target_col), lit(\"Null\")),\n",
    "                    lit(\"changed to new value:\"),\n",
    "                    coalesce(col(source_col), lit(\"Null\")),\n",
    "                ),\n",
    "            ).otherwise(lit(\"\"))\n",
    "            change_conditions.append(change_condition)\n",
    "        new_record_condition = when(\n",
    "            col(\"target_icn\").isNull(), lit(\"New Record\")\n",
    "        ).otherwise(lit(\"Updated Record\"))\n",
    "        upsert_df = upsert_df.withColumn(\"RecordChangeStatus\", new_record_condition)\n",
    "        if len(change_conditions) > 0:\n",
    "            change_log_col = concat_ws(\n",
    "                \" \", *[coalesce(cond, lit(\"\")) for cond in change_conditions]\n",
    "            )\n",
    "        else:\n",
    "            change_log_col = lit(\"\")\n",
    "        upsert_df = upsert_df.withColumn(\"change_log\", change_log_col)\n",
    "        if file_type == \"PAI\":\n",
    "            file_type = \"SCD\"\n",
    "        # Perform the merge operation\n",
    "        targetTable.alias(\"target\").merge(\n",
    "            upsert_df.alias(\"source\"), merge_conditions[file_type]\n",
    "        ).whenMatchedUpdate(\n",
    "            set={\n",
    "                \"RecordStatus\": \"False\",\n",
    "                \"RecordLastUpdated\": \"source.SDP_Event_Created_Timestamp\",\n",
    "                \"sentToDoD\": \"true\",\n",
    "                \"RecordChangeStatus\": lit(\"Expired Record\"),\n",
    "            }\n",
    "        ).whenNotMatchedInsert(\n",
    "            values={\n",
    "                \"edipi\": \"source.edipi\",\n",
    "                \"ICN\": \"source.ICN\",\n",
    "                \"Veteran_ICN\": \"source.Veteran_ICN\",\n",
    "                \"Applicant_Type\": \"source.Applicant_Type\",\n",
    "                \"Caregiver_Status\": \"source.Caregiver_Status\",\n",
    "                \"participant_id\": \"source.participant_id\",\n",
    "                \"Batch_CD\": \"source.Batch_CD\",\n",
    "                \"SC_Combined_Disability_Percentage\": \"source.SC_Combined_Disability_Percentage\",\n",
    "                \"PT_Indicator\": \"source.PT_Indicator\",\n",
    "                \"Individual_Unemployability\": \"source.Individual_Unemployability\",\n",
    "                \"Status_Begin_Date\": \"source.Status_Begin_Date\",\n",
    "                \"Status_Last_Update\": \"source.Status_Last_Update\",\n",
    "                \"Status_Termination_Date\": \"source.Status_Termination_Date\",\n",
    "                \"SDP_Event_Created_Timestamp\": \"source.SDP_Event_Created_Timestamp\",\n",
    "                \"RecordStatus\": \"true\",\n",
    "                \"RecordLastUpdated\": \"source.RecordLastUpdated\",\n",
    "                \"filename\": \"source.filename\",\n",
    "                \"sentToDoD\": \"false\",\n",
    "                \"change_log\": \"source.change_log\",\n",
    "                \"RecordChangeStatus\": \"source.RecordChangeStatus\",\n",
    "            }\n",
    "        ).execute()\n",
    "\n",
    "    def prepare_caregivers_data(self, cg_csv_files):\n",
    "        \"\"\"\n",
    "        Filters caregivers filenames from input dataframe, aggregates data and returns dataframe\n",
    "        Parameters: Pyspark dataframe with all filenames and metadata that are not processed (upsert)\n",
    "        Returns: Dataframe: Dataframe with required column names and edipi of a caregiver ready for upsert\n",
    "        \"\"\"\n",
    "        print(\n",
    "            f\"Upserting records from {cg_csv_files.count()} caregivers aggregated files\"\n",
    "        )\n",
    "        Window_Spec = Window.partitionBy(\n",
    "            \"ICN\", \"Veteran_ICN\", \"Applicant_Type\"\n",
    "        ).orderBy(desc(\"Event_Created_Date\"))\n",
    "\n",
    "        cg_csv_files_to_process = cg_csv_files.select(collect_list(\"path\")).first()[0]\n",
    "        cg_df = (\n",
    "            spark.read.schema(new_cg_schema)\n",
    "            .csv(cg_csv_files_to_process, header=True, inferSchema=False)\n",
    "            .selectExpr(\"*\", \"_metadata.file_name as filename\", \"_metadata.file_modification_time as SDP_Event_Created_Timestamp \" )\n",
    "        )\n",
    "        combined_cg_df = (\n",
    "            cg_df.select(\n",
    "                substring(\"Caregiver_ICN__c\", 1, 10).alias(\"ICN\"),\n",
    "                substring(\"Veteran_ICN__c\", 1, 10).alias(\"Veteran_ICN\"),\n",
    "                date_format(\"Benefits_End_Date__c\", \"yyyyMMdd\")\n",
    "                .alias(\"Status_Termination_Date\")\n",
    "                .cast(StringType()),\n",
    "                col(\"Applicant_Type__c\").alias(\"Applicant_Type\"),\n",
    "                col(\"Caregiver_Status__c\").alias(\"Caregiver_Status\"),\n",
    "                date_format(\"Dispositioned_Date__c\", \"yyyyMMdd\")\n",
    "                .alias(\"Status_Begin_Date\")\n",
    "                .cast(StringType()),\n",
    "                col(\"CreatedDate\").cast(\"timestamp\").alias(\"Event_Created_Date\"),\n",
    "                \"filename\",\n",
    "                \"SDP_Event_Created_Timestamp\",\n",
    "            )\n",
    "        ).filter(col(\"Caregiver_ICN__c\").isNotNull())\n",
    "        edipi_df = (\n",
    "            broadcast(combined_cg_df)\n",
    "            .join(self.icn_relationship, [\"ICN\"], \"left\")\n",
    "            .withColumn(\"Individual_Unemployability\", lit(None).cast(StringType()))\n",
    "            .withColumn(\"PT_Indicator\", lit(None).cast(StringType()))\n",
    "            .withColumn(\n",
    "                \"SC_Combined_Disability_Percentage\", lit(None).cast(StringType())\n",
    "            )\n",
    "            .withColumn(\"RecordStatus\", lit(True).cast(BooleanType()))\n",
    "            .withColumn(\"RecordLastUpdated\", lit(None).cast(DateType()))\n",
    "            .withColumn(\"Status_Last_Update\", lit(None).cast(StringType()))\n",
    "            .withColumn(\"sentToDoD\", lit(False).cast(BooleanType()))\n",
    "            .withColumn(\"Batch_CD\", lit(\"CG\").cast(StringType()))\n",
    "            .withColumn(\"rank\", rank().over(Window_Spec))\n",
    "            .filter(col(\"rank\") == 1)\n",
    "            .dropDuplicates()\n",
    "            .drop(\"rank\", \"va_profile_id\", \"record_updated_date\")\n",
    "        ).orderBy(col(\"Event_Created_Date\"))\n",
    "        return edipi_df\n",
    "\n",
    "    def prepare_scd_data(self, row):\n",
    "        \"\"\"\n",
    "        Prepares SCD data from the input row. This is the disability % data.\n",
    "        Parameters: Row of data from pyspark dataframe with filenames and metadata that are not processed (upsert)\n",
    "        Returns: Dataframe: Dataframe with required column names and edipi of a Veteran ready for upsert\n",
    "        \"\"\"\n",
    "\n",
    "        file_name = row.path\n",
    "\n",
    "        print(f\"Upserting records from {file_name}\")\n",
    "\n",
    "        # if len(spark.read.csv(file_name).columns) != 3:\n",
    "        #     schema = scd_schema\n",
    "        # else:\n",
    "        #     schema = scd_schema1\n",
    "        scd_updates_df = (\n",
    "            spark.read.csv(file_name, schema=scd_schema1, header=True, inferSchema=False)\n",
    "            .selectExpr(\n",
    "                \"PTCPNT_ID as participant_id\", \"CMBNED_DEGREE_DSBLTY\", \"DSBL_DTR_DT\", \"_metadata.file_name as filename\", \"_metadata.file_modification_time as SDP_Event_Created_Timestamp \"\n",
    "            )\n",
    "            .withColumn(\"sentToDoD\", lit(False).cast(BooleanType()))\n",
    "            .withColumn(\n",
    "                \"SC_Combined_Disability_Percentage\",\n",
    "                lpad(\n",
    "                    coalesce(\n",
    "                        when(col(\"CMBNED_DEGREE_DSBLTY\") == \"\", lit(\"000\")).otherwise(\n",
    "                            col(\"CMBNED_DEGREE_DSBLTY\")\n",
    "                        )\n",
    "                    ),\n",
    "                    3,\n",
    "                    \"0\",\n",
    "                ),\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"DSBL_DTR_DT\",\n",
    "                when(col(\"DSBL_DTR_DT\") == \"\", None).otherwise(\n",
    "                    date_format(to_date(col(\"DSBL_DTR_DT\"), \"MMddyyyy\"), \"yyyyMMdd\")\n",
    "                ),\n",
    "            )\n",
    "        ).filter(col(\"DSBL_DTR_DT\").isNotNull())\n",
    "\n",
    "        Window_Spec = Window.partitionBy(scd_updates_df[\"participant_id\"]).orderBy(\n",
    "            desc(\"DSBL_DTR_DT\"), desc(\"SC_Combined_Disability_Percentage\")\n",
    "        )\n",
    "        edipi_df = (\n",
    "            broadcast(scd_updates_df)\n",
    "            .join(self.icn_relationship, [\"participant_id\"], \"left\")\n",
    "            .withColumn(\"rank\", rank().over(Window_Spec))\n",
    "            .withColumn(\"Veteran_ICN\", lit(None).cast(StringType()))\n",
    "            .withColumn(\"Applicant_Type\", lit(None).cast(StringType()))\n",
    "            .withColumn(\"Caregiver_Status\", lit(None).cast(StringType()))\n",
    "            .withColumn(\"Individual_Unemployability\", lit(None).cast(StringType()))\n",
    "            .withColumn(\"Status_Termination_Date\", lit(None).cast(StringType()))\n",
    "            .withColumn(\"RecordLastUpdated\", lit(None).cast(DateType()))\n",
    "            .withColumn(\"Batch_CD\", lit(\"SCD\"))\n",
    "            .withColumn(\"RecordStatus\", lit(True).cast(BooleanType()))\n",
    "            .filter(col(\"rank\") == 1)\n",
    "            .filter(col(\"ICN\").isNotNull())\n",
    "            .dropDuplicates()\n",
    "            .drop(\"rank\", \"va_profile_id\", \"record_updated_date\")\n",
    "        )\n",
    "\n",
    "        return edipi_df\n",
    "\n",
    "    def update_pai_data(self, row, source_type):\n",
    "        \"\"\"\n",
    "        Prepares PT Indicator from the input row, transforms and updates a Veteran's PT_Indicator column in delta table.\n",
    "        Parameters: Row of data from pyspark dataframe with metadata that are not processed (upsert),\n",
    "                source_type is 'text' means its a static file (old source)\n",
    "                source_type is 'table' means its a delta table (new source)\n",
    "        Returns: Dataframe: Dataframe with required column names ready for upsert\n",
    "        \"\"\"\n",
    "        # if source_type == \"text\":\n",
    "        #     file_name = row.path\n",
    "        #     file_creation_dateTime = row.dateTime\n",
    "        #     raw_pai_df = (\n",
    "        #         spark.read.csv(file_name, header=True, inferSchema=True)\n",
    "        #         .selectExpr(\"*\", \"_metadata.file_name as filename\", \"_metadata.file_modification_time as SDP_Event_Created_Timestamp \" )\n",
    "        #     )\n",
    "        # elif source_type == \"table\":\n",
    "        file_creation_dateTime = row.dateTime\n",
    "        file_name = f\"Updated from PA&I delta table on {file_creation_dateTime}\"\n",
    "        raw_pai_df = spark.read.format(\"delta\").load(self.pt_new_source)\n",
    "\n",
    "        print(f\"Updating PT Indicator\")\n",
    "\n",
    "        targetTable = DeltaTable.forPath(\n",
    "            spark, self.fullname\n",
    "        )\n",
    "        targetDF = (\n",
    "            targetTable.toDF()\n",
    "            .filter(\"Batch_CD == 'SCD'\")\n",
    "            .filter(\"RecordStatus=='True'\")\n",
    "        )\n",
    "        targetDF = targetDF.select(\n",
    "            [col(c).alias(f\"target_{c}\") for c in targetDF.columns]\n",
    "        )\n",
    "\n",
    "        pai_df = raw_pai_df.selectExpr(\n",
    "            \"PTCPNT_VET_ID as participant_id\", \"PT_35_FLAG as source_PT_Indicator\"\n",
    "        )\n",
    "        existing_pai_data = f\"\"\"SELECT participant_id, PT_Indicator from {self.patronage_tablename} where RecordStatus is True and Batch_CD = 'SCD' \"\"\"\n",
    "        existing_pai_data_df = spark.sql(existing_pai_data)\n",
    "\n",
    "        joinDF = (\n",
    "            pai_df.join(\n",
    "                broadcast(targetDF),\n",
    "                pai_df[\"participant_id\"] == targetDF[\"target_participant_id\"],\n",
    "                \"left\",\n",
    "            )\n",
    "            .filter(targetDF[\"target_PT_Indicator\"] == \"N\")\n",
    "            .withColumn(\"filename\", lit(file_name))\n",
    "            .withColumn(\"SDP_Event_Created_Timestamp\", lit(file_creation_dateTime))\n",
    "        )\n",
    "\n",
    "        filterDF = joinDF.filter(\n",
    "            xxhash64(joinDF.source_PT_Indicator) != xxhash64(joinDF.target_PT_Indicator)\n",
    "        )\n",
    "\n",
    "        mergeDF = filterDF.withColumn(\"MERGEKEY\", filterDF.target_ICN)\n",
    "\n",
    "        dummyDF = filterDF.filter(\"target_ICN is not null\").withColumn(\n",
    "            \"MERGEKEY\", lit(None)\n",
    "        )\n",
    "\n",
    "        paiDF = mergeDF.union(dummyDF)\n",
    "        edipi_df = (\n",
    "            paiDF.selectExpr(\n",
    "                \"target_edipi as edipi\",\n",
    "                \"participant_id\",\n",
    "                \"target_ICN\",\n",
    "                \"MERGEKEY\",\n",
    "                \"target_SC_Combined_Disability_Percentage as SC_Combined_Disability_Percentage\",\n",
    "                \"target_Status_Begin_Date as Status_Begin_Date\",\n",
    "                \"target_Status_Last_Update as Status_Last_Update\",\n",
    "                \"SDP_Event_Created_Timestamp\",\n",
    "                \"filename\",\n",
    "                \"source_PT_Indicator\",\n",
    "                \"target_PT_Indicator\",\n",
    "            )\n",
    "            .withColumn(\"ICN\", lit(col(\"target_ICN\")))\n",
    "            .withColumn(\"Veteran_ICN\", lit(None))\n",
    "            .withColumn(\"Applicant_Type\", lit(None))\n",
    "            .withColumn(\"Caregiver_Status\", lit(None))\n",
    "            .withColumn(\"Batch_CD\", lit(\"SCD\"))\n",
    "            .withColumn(\"PT_Indicator\", coalesce(col(\"source_PT_Indicator\"), lit(\"N\")))\n",
    "            .withColumn(\"Individual_Unemployability\", lit(None))\n",
    "            .withColumn(\"Status_Termination_Date\", lit(None))\n",
    "            .withColumn(\"RecordLastUpdated\", lit(None))\n",
    "        )\n",
    "        return edipi_df\n",
    "\n",
    "    def process_files(self, files_to_process_now):\n",
    "        \"\"\"\n",
    "        Segregates files based on filename and calls required function to process these files\n",
    "        Parameters: Pyspark dataframe with filenames and metadata that are not processed\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        cg_csv_files = files_to_process_now.filter(\n",
    "            files_to_process_now[\"path\"].contains(\"caregiverevent\")\n",
    "        )\n",
    "        edipi_df = self.prepare_caregivers_data(cg_csv_files)\n",
    "        self.process_updates(edipi_df, \"CG\")\n",
    "\n",
    "        other_files = files_to_process_now.filter(\n",
    "            ~files_to_process_now[\"path\"].contains(\"caregiverevent\")\n",
    "        )\n",
    "        other_rows = other_files.collect()\n",
    "\n",
    "        for row in other_rows:\n",
    "            filename = row.path\n",
    "            if \"CPIDODIEX\" in filename:\n",
    "                edipi_df = self.prepare_scd_data(row)\n",
    "                self.process_updates(edipi_df, \"SCD\")\n",
    "            elif \"WRTS\" in filename:\n",
    "                edipi_df = self.update_pai_data(row, \"text\")\n",
    "                self.process_updates(edipi_df, \"PAI\")\n",
    "            elif \"parquet\" in filename:\n",
    "                edipi_df = self.update_pai_data(row, \"table\")\n",
    "                self.process_updates(edipi_df, \"PAI\")\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e4c0f21-05ae-46ca-92d8-879322421916",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Instantiating FileProcessor Class"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    file_processor = FileProcessor()\n",
    "\n",
    "    if file_processor.no_of_files == 0:\n",
    "        print(f\"Loading Caregivers Seed File into the {patronage_tablename} \")\n",
    "        seed_df = file_processor.initialize_caregivers()\n",
    "        file_processor.process_updates(seed_df, \"CG\")\n",
    "        print(\"Loading required files for upsert\")\n",
    "        files_to_process_now = file_processor.collect_data_source()\n",
    "        display(files_to_process_now)\n",
    "        print(f\"Total files to process in this run: {files_to_process_now.count()}...\")\n",
    "        file_processor.process_files(files_to_process_now)\n",
    "    else:\n",
    "        print(\"Loading required files for upsert\")\n",
    "        files_to_process_now = file_processor.collect_data_source()\n",
    "        display(files_to_process_now)\n",
    "        print(f\"Total files to process in this run: {files_to_process_now.count()}...\")\n",
    "        file_processor.process_files(files_to_process_now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7db3bfe-4b0d-454d-b8c2-5ae053bf1e51",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Main method to run the pipeline"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a48f119c-9106-403b-8c44-404fe784dace",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_processor = FileProcessor()\n",
    "display(file_processor.collect_data_source())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "447bbf08-2630-4bf8-80d2-b32f74a7aeaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  version,\n",
    "  timestamp,\n",
    "  operation,\n",
    "  cast(operationMetrics.numTargetRowsInserted as INT) as TotalInsertedRecords,\n",
    "  cast(operationMetrics.numTargetRowsUpdated as INT) as UpdatedRecords,\n",
    "    (cast(operationMetrics.numTargetRowsInserted as INT) -  cast(operationMetrics.numTargetRowsUpdated as INT)) as NewRecords\n",
    "\n",
    "FROM\n",
    "  (describe history mypat_test)\n",
    "-- where\n",
    "--   operation not in (\"CREATE TABLE\", \"OPTIMIZE\")\n",
    "order by\n",
    "  1 desc;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c42b06-c57f-48d9-8c98-6f683faeeb0a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Number of records per processed day"
    }
   },
   "outputs": [],
   "source": [
    "recordsPerDate = f\"\"\" select count(*) as NoOfRecords, SDP_Event_Created_Timestamp \n",
    "                      from {file_processor.patronage_tablename}\n",
    "                      group by all\n",
    "                      order by 2 desc\"\"\"\n",
    "\n",
    "spark.sql(recordsPerDate).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7596f45-6117-4ae7-bd35-0e6533ad1a96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# DROP TABLE mypat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "903b8b53-2210-40db-99f0-e52c02c4e6e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.rm(\"dbfs:/user/hive/warehouse/mypat_test\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3b32907-7f89-487c-ad41-d7fe62b07ab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# SELECT count(*), ICN, Veteran_ICN, Applicant_Type FROM mypat_test\n",
    "# WHERE RecordStatus IS TRUE\n",
    "# GROUP BY ALL\n",
    "# HAVING count(*) > 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b8e0418-8101-4384-8e1b-e750d81e8cf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(*) from pai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff6b7d1c-36de-443c-8737-76ffc4d62713",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(*) from may_full_pai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f068c1d-e36b-4ea2-a401-44be34cceb39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count( distinct PTCPNT_VET_ID) from pai"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6983560865755057,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4,
    "widgetLayout": []
   },
   "notebookName": "Patronage V4",
   "widgets": {
    "CaregiverSource": {
     "currentValue": "/mnt/ci-carma/landing/",
     "nuid": "e3628b61-6186-4ded-9df7-0bb88308a434",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/mnt/ci-carma/landing/",
      "label": "Caregiver Source",
      "name": "CaregiverSource",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/mnt/ci-carma/landing/",
      "label": "Caregiver Source",
      "name": "CaregiverSource",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "DisabilityPercentSource": {
     "currentValue": "/mnt/ci-vadir-shared/",
     "nuid": "8702b903-c77d-42c5-adf6-6995364e2a25",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/mnt/ci-vadir-shared/",
      "label": "Disability Percent Source",
      "name": "DisabilityPercentSource",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/mnt/ci-vadir-shared/",
      "label": "Disability Percent Source",
      "name": "DisabilityPercentSource",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "NewPTIndicatorSource": {
     "currentValue": "/mnt/ci-vba-edw-2/DeltaTables/DW_ADHOC_RECURR.DOD_PATRONAGE_SCD_PT/",
     "nuid": "06213c06-bf2a-4560-b380-dfe4d0f47911",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/mnt/ci-vba-edw-2/DeltaTables/DW_ADHOC_RECURR.DOD_PATRONAGE_SCD_PT/",
      "label": "New PT Indicator Source",
      "name": "NewPTIndicatorSource",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/mnt/ci-vba-edw-2/DeltaTables/DW_ADHOC_RECURR.DOD_PATRONAGE_SCD_PT/",
      "label": "New PT Indicator Source",
      "name": "NewPTIndicatorSource",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "OldPTIndicatorSource": {
     "currentValue": "/mnt/ci-patronage/pai_landing/",
     "nuid": "755b43ce-7a51-47fa-8998-d612c2fc594b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/mnt/ci-patronage/pai_landing/",
      "label": "Old PT Indicator Source",
      "name": "OldPTIndicatorSource",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/mnt/ci-patronage/pai_landing/",
      "label": "Old PT Indicator Source",
      "name": "OldPTIndicatorSource",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "PathToIdentityCorrelationsDeltaTable": {
     "currentValue": "/mnt/ci-patronage/delta_tables/correlation_lookup",
     "nuid": "c686e62d-a912-44dc-91ff-02f870f988e5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/mnt/Patronage/identity_correlations",
      "label": "Path to Identity Correlations Delta Table",
      "name": "PathToIdentityCorrelationsDeltaTable",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/mnt/Patronage/identity_correlations",
      "label": "Path to Identity Correlations Delta Table",
      "name": "PathToIdentityCorrelationsDeltaTable",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "PathToInitialSeedCaregiversCSVFile": {
     "currentValue": "dbfs:/FileStore/All_Caregivers_InitialSeed_12182024_csv.csv",
     "nuid": "a3f7d8f7-00bc-4096-b3bc-7dc3cc3cbe36",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dbfs:/FileStore/All_Caregivers_InitialSeed_12182024_csv.csv",
      "label": "Path to initial seed Caregivers csv file",
      "name": "PathToInitialSeedCaregiversCSVFile",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dbfs:/FileStore/All_Caregivers_InitialSeed_12182024_csv.csv",
      "label": "Path to initial seed Caregivers csv file",
      "name": "PathToInitialSeedCaregiversCSVFile",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "PatronageDeltaTableName": {
     "currentValue": "mypat_test",
     "nuid": "3a926262-b9a1-4232-9a7f-792f9c80951b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "mypat_test",
      "label": "Patronage Delta Table Name",
      "name": "PatronageDeltaTableName",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "mypat_test",
      "label": "Patronage Delta Table Name",
      "name": "PatronageDeltaTableName",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "PatronageDeltaTablePath": {
     "currentValue": "dbfs:/user/hive/warehouse/",
     "nuid": "30c984c7-b976-4121-8f30-0b32ff8c9673",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dbfs:/user/hive/warehouse/",
      "label": "Patronage Delta Table Path",
      "name": "PatronageDeltaTablePath",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dbfs:/user/hive/warehouse/",
      "label": "Patronage Delta Table Path",
      "name": "PatronageDeltaTablePath",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "StartDateToProcessCaregiversData": {
     "currentValue": "2024-12-18 23:59:59",
     "nuid": "c3da3789-e2e4-4eee-b046-b4be03a63007",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2024-12-18 23:59:59",
      "label": "Start Date to Process Caregivers Data",
      "name": "StartDateToProcessCaregiversData",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2024-12-18 23:59:59",
      "label": "Start Date to Process Caregivers Data",
      "name": "StartDateToProcessCaregiversData",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "StartDateToProcessSCDData": {
     "currentValue": "2025-06-01 00:00:00",
     "nuid": "68e8aa64-75ef-47e7-afb7-1dd4bee3f985",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2025-06-01 00:00:00",
      "label": "Start Date to Process SCD Data",
      "name": "StartDateToProcessSCDData",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2025-06-01 00:00:00",
      "label": "Start Date to Process SCD Data",
      "name": "StartDateToProcessSCDData",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "rebuild": {
     "currentValue": "False",
     "nuid": "9d77865f-9272-4d55-98ff-865a8bf1f2c6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "False",
      "label": "1 Rebuild Patronage",
      "name": "rebuild",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "True",
        "False"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "False",
      "label": "1 Rebuild Patronage",
      "name": "rebuild",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "True",
        "False"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
