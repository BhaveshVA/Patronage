{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "910e1985-bc14-4c8d-b75b-04c412547c25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Dynamic Data Source Column Validation\n",
    "\n",
    "## Purpose\n",
    "This notebook provides a flexible, automated way to validate that specific columns exist in one or more data sources (CSV or Delta tables) using PySpark and Python's `unittest` framework. It is designed for data engineers and analysts who need to ensure that ingested or processed files conform to expected schemas before further processing or reporting.\n",
    "\n",
    "## How to Use\n",
    "1. **Edit the `data_sources` list at the top of the code cell:**\n",
    "   - Each entry should specify:\n",
    "     - `name`: A descriptive name for the data source (for reporting and error messages).\n",
    "     - `file_type`: Either `'csv'` or `'delta'`.\n",
    "     - `file_path`: The full path to the file or Delta table.\n",
    "     - `expected_columns`: A list of column names that must be present in the data source.\n",
    "   - You can add, remove, or comment out entries as needed to check any combination of data sources. The code will only check the sources you include in the list.\n",
    "\n",
    "2. **Run the notebook:**\n",
    "   - The test will loop through all specified data sources and check for the presence of the required columns.\n",
    "   - For each data source, the test prints which file is being checked and reports any missing columns with clear error messages, including the file name and which columns are missing.\n",
    "\n",
    "3. **Interpreting Results:**\n",
    "   - If all columns are present in all data sources, the test passes and you will see confirmation for each file checked.\n",
    "   - If any columns are missing or a file cannot be read, the test fails and prints a detailed message indicating which file and which columns are problematic.\n",
    "   - If any test fails, the notebook will exit with a failure message (if run in Databricks or similar environments).\n",
    "\n",
    "## Example\n",
    "To check only one file, simply leave only one entry in the `data_sources` list. To check two or more, add them to the list. The code is fully dynamic and does not require changes to the test logic for different files or schemas.\n",
    "\n",
    "**Tip:** You can use this notebook as a template for any schema validation task by simply updating the `data_sources` list to match your current validation needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d431b30a-7f1d-4a94-8786-e2dcc3a13520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# Define your data sources here. Add or remove entries as needed.\n",
    "data_sources = [\n",
    "    {\n",
    "        \"name\": \"Caregivers CSV\",\n",
    "        \"file_type\": \"csv\",\n",
    "        \"file_path\": \"dbfs:/mnt/ci-carma/landing/caregiverevent-3531dc00-4bb1-11f0-8e22-065857d19e8f.csv\",\n",
    "        \"expected_columns\": [\n",
    "            \"Caregiver_ICN__c\", \"Applicant_Type__c\", \"Caregiver_Status__c\", \n",
    "            \"Dispositioned_Date__c\", \"Benefits_End_Date__c\", \"Veteran_ICN__c\", \"CreatedDate\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Disability CSV\",\n",
    "        \"file_type\": \"csv\",\n",
    "        \"file_path\": \"dbfs:/mnt/ci-vadir-shared/CPIDODIEX_20250618_spool.csv\",\n",
    "        \"expected_columns\": [\"PTCPNT_ID\", \"CMBNED_DEGREE_DSBLTY\", \"DSBL_DTR_DT\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"PT Indicator Delta\",\n",
    "        \"file_type\": \"delta\",\n",
    "        \"file_path\": \"/mnt/ci-vba-edw-2/DeltaTables/DW_ADHOC_RECURR.DOD_PATRONAGE_SCD_PT/\",\n",
    "        \"expected_columns\": [\"PTCPNT_VET_ID\", \"PT_35_FLAG\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "class TestFileColumns(unittest.TestCase):\n",
    "    def check_columns(self, file_type, file_path, expected_columns, name=None):\n",
    "        print(f\"Checking file: {file_path} (type: {file_type})\" + (f\" [{name}]\" if name else \"\"))\n",
    "        try:\n",
    "            if file_type == 'csv':\n",
    "                df = spark.read.csv(file_path, header=True)\n",
    "            elif file_type == 'delta':\n",
    "                df = spark.read.format(\"delta\").load(file_path)\n",
    "            else:\n",
    "                self.fail(f\"Unknown file type: {file_type} for file {file_path}\")\n",
    "            missing_columns = set(expected_columns) - set(df.columns)\n",
    "            self.assertTrue(\n",
    "                len(missing_columns) == 0,\n",
    "                f\"[{name}] File {file_path} is missing expected columns: {missing_columns}. Found columns: {df.columns}\"\n",
    "            )\n",
    "        except AnalysisException as e:\n",
    "            self.fail(f\"[{name}] File {file_path} could not be read: {str(e)}\")\n",
    "\n",
    "    def test_dynamic_data_sources(self):\n",
    "        for ds in data_sources:\n",
    "            with self.subTest(data_source=ds[\"name\"]):\n",
    "                self.check_columns(\n",
    "                    ds[\"file_type\"],\n",
    "                    ds[\"file_path\"],\n",
    "                    ds[\"expected_columns\"],\n",
    "                    name=ds[\"name\"]\n",
    "                )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    result = unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
    "    if not result.result.wasSuccessful():\n",
    "        print(\"Unit tests failed. Exiting the notebook.\")\n",
    "        dbutils.notebook.exit(\"Tests failed, exiting notebook and failing job.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "validate_patronage_columns.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
